{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>七彩椒(1)</th>\n",
       "      <th>七彩椒(2)</th>\n",
       "      <th>上海青</th>\n",
       "      <th>东门口小白菜</th>\n",
       "      <th>云南油麦菜</th>\n",
       "      <th>云南生菜</th>\n",
       "      <th>云南生菜(份)</th>\n",
       "      <th>余干椒</th>\n",
       "      <th>冰草</th>\n",
       "      <th>净藕(1)</th>\n",
       "      <th>...</th>\n",
       "      <th>青杭椒(2)</th>\n",
       "      <th>青梗散花</th>\n",
       "      <th>青线椒</th>\n",
       "      <th>青茄子(1)</th>\n",
       "      <th>高瓜(1)</th>\n",
       "      <th>鲜木耳(1)</th>\n",
       "      <th>鲜木耳(份)</th>\n",
       "      <th>黄心菜(1)</th>\n",
       "      <th>黄心菜(2)</th>\n",
       "      <th>黄白菜(2)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-07-01</th>\n",
       "      <td>11.283125</td>\n",
       "      <td>17.223246</td>\n",
       "      <td>4.448322</td>\n",
       "      <td>3.558951</td>\n",
       "      <td>4.288465</td>\n",
       "      <td>4.892639</td>\n",
       "      <td>3.156615</td>\n",
       "      <td>9.001972</td>\n",
       "      <td>9.141936</td>\n",
       "      <td>7.593090</td>\n",
       "      <td>...</td>\n",
       "      <td>6.633472</td>\n",
       "      <td>6.327193</td>\n",
       "      <td>8.030885</td>\n",
       "      <td>2.451122</td>\n",
       "      <td>10.695603</td>\n",
       "      <td>7.450570</td>\n",
       "      <td>1.269304</td>\n",
       "      <td>3.342631</td>\n",
       "      <td>3.572313</td>\n",
       "      <td>4.398471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-02</th>\n",
       "      <td>11.555013</td>\n",
       "      <td>17.286523</td>\n",
       "      <td>4.577368</td>\n",
       "      <td>3.703236</td>\n",
       "      <td>4.343757</td>\n",
       "      <td>4.982323</td>\n",
       "      <td>3.160196</td>\n",
       "      <td>11.219541</td>\n",
       "      <td>9.274606</td>\n",
       "      <td>7.332504</td>\n",
       "      <td>...</td>\n",
       "      <td>6.788234</td>\n",
       "      <td>6.232115</td>\n",
       "      <td>8.045755</td>\n",
       "      <td>2.528307</td>\n",
       "      <td>10.729535</td>\n",
       "      <td>7.467691</td>\n",
       "      <td>1.282147</td>\n",
       "      <td>3.227073</td>\n",
       "      <td>3.531505</td>\n",
       "      <td>4.456539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-03</th>\n",
       "      <td>11.877639</td>\n",
       "      <td>17.571826</td>\n",
       "      <td>4.558968</td>\n",
       "      <td>3.543832</td>\n",
       "      <td>4.274410</td>\n",
       "      <td>4.952225</td>\n",
       "      <td>3.096889</td>\n",
       "      <td>12.319600</td>\n",
       "      <td>9.393054</td>\n",
       "      <td>7.025901</td>\n",
       "      <td>...</td>\n",
       "      <td>6.831326</td>\n",
       "      <td>6.231063</td>\n",
       "      <td>7.845660</td>\n",
       "      <td>2.493652</td>\n",
       "      <td>10.709710</td>\n",
       "      <td>7.468844</td>\n",
       "      <td>1.273445</td>\n",
       "      <td>3.214212</td>\n",
       "      <td>3.444150</td>\n",
       "      <td>4.449769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-04</th>\n",
       "      <td>11.420698</td>\n",
       "      <td>18.210313</td>\n",
       "      <td>4.559555</td>\n",
       "      <td>3.516277</td>\n",
       "      <td>4.295227</td>\n",
       "      <td>4.932869</td>\n",
       "      <td>3.020461</td>\n",
       "      <td>13.788977</td>\n",
       "      <td>9.360426</td>\n",
       "      <td>6.748812</td>\n",
       "      <td>...</td>\n",
       "      <td>6.916969</td>\n",
       "      <td>6.223693</td>\n",
       "      <td>7.936348</td>\n",
       "      <td>2.479481</td>\n",
       "      <td>10.753400</td>\n",
       "      <td>7.424200</td>\n",
       "      <td>1.285398</td>\n",
       "      <td>3.169600</td>\n",
       "      <td>3.391892</td>\n",
       "      <td>4.500709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-05</th>\n",
       "      <td>11.405392</td>\n",
       "      <td>18.708175</td>\n",
       "      <td>4.642817</td>\n",
       "      <td>3.744003</td>\n",
       "      <td>4.316992</td>\n",
       "      <td>4.958944</td>\n",
       "      <td>2.892764</td>\n",
       "      <td>14.328330</td>\n",
       "      <td>9.473411</td>\n",
       "      <td>6.433894</td>\n",
       "      <td>...</td>\n",
       "      <td>6.926025</td>\n",
       "      <td>6.105857</td>\n",
       "      <td>8.017064</td>\n",
       "      <td>2.529456</td>\n",
       "      <td>10.939844</td>\n",
       "      <td>7.369914</td>\n",
       "      <td>1.300645</td>\n",
       "      <td>3.174790</td>\n",
       "      <td>3.274633</td>\n",
       "      <td>4.473733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-06</th>\n",
       "      <td>11.631606</td>\n",
       "      <td>19.020030</td>\n",
       "      <td>4.736267</td>\n",
       "      <td>3.683155</td>\n",
       "      <td>4.248266</td>\n",
       "      <td>4.927594</td>\n",
       "      <td>2.929317</td>\n",
       "      <td>14.765293</td>\n",
       "      <td>9.323351</td>\n",
       "      <td>6.053643</td>\n",
       "      <td>...</td>\n",
       "      <td>7.135817</td>\n",
       "      <td>6.126682</td>\n",
       "      <td>7.856343</td>\n",
       "      <td>2.530739</td>\n",
       "      <td>10.909184</td>\n",
       "      <td>7.362216</td>\n",
       "      <td>1.299923</td>\n",
       "      <td>3.133978</td>\n",
       "      <td>3.299617</td>\n",
       "      <td>4.509486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-07</th>\n",
       "      <td>12.235706</td>\n",
       "      <td>19.603635</td>\n",
       "      <td>4.667127</td>\n",
       "      <td>3.635333</td>\n",
       "      <td>4.305486</td>\n",
       "      <td>4.672407</td>\n",
       "      <td>2.788980</td>\n",
       "      <td>13.550692</td>\n",
       "      <td>9.308037</td>\n",
       "      <td>5.701222</td>\n",
       "      <td>...</td>\n",
       "      <td>7.045419</td>\n",
       "      <td>6.119803</td>\n",
       "      <td>8.271865</td>\n",
       "      <td>2.452341</td>\n",
       "      <td>10.770628</td>\n",
       "      <td>7.423415</td>\n",
       "      <td>1.301105</td>\n",
       "      <td>3.050977</td>\n",
       "      <td>3.204841</td>\n",
       "      <td>4.465784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               七彩椒(1)     七彩椒(2)       上海青    东门口小白菜     云南油麦菜      云南生菜  \\\n",
       "Unnamed: 0                                                                 \n",
       "2023-07-01  11.283125  17.223246  4.448322  3.558951  4.288465  4.892639   \n",
       "2023-07-02  11.555013  17.286523  4.577368  3.703236  4.343757  4.982323   \n",
       "2023-07-03  11.877639  17.571826  4.558968  3.543832  4.274410  4.952225   \n",
       "2023-07-04  11.420698  18.210313  4.559555  3.516277  4.295227  4.932869   \n",
       "2023-07-05  11.405392  18.708175  4.642817  3.744003  4.316992  4.958944   \n",
       "2023-07-06  11.631606  19.020030  4.736267  3.683155  4.248266  4.927594   \n",
       "2023-07-07  12.235706  19.603635  4.667127  3.635333  4.305486  4.672407   \n",
       "\n",
       "             云南生菜(份)        余干椒        冰草     净藕(1)  ...    青杭椒(2)      青梗散花  \\\n",
       "Unnamed: 0                                           ...                       \n",
       "2023-07-01  3.156615   9.001972  9.141936  7.593090  ...  6.633472  6.327193   \n",
       "2023-07-02  3.160196  11.219541  9.274606  7.332504  ...  6.788234  6.232115   \n",
       "2023-07-03  3.096889  12.319600  9.393054  7.025901  ...  6.831326  6.231063   \n",
       "2023-07-04  3.020461  13.788977  9.360426  6.748812  ...  6.916969  6.223693   \n",
       "2023-07-05  2.892764  14.328330  9.473411  6.433894  ...  6.926025  6.105857   \n",
       "2023-07-06  2.929317  14.765293  9.323351  6.053643  ...  7.135817  6.126682   \n",
       "2023-07-07  2.788980  13.550692  9.308037  5.701222  ...  7.045419  6.119803   \n",
       "\n",
       "                 青线椒    青茄子(1)      高瓜(1)    鲜木耳(1)    鲜木耳(份)    黄心菜(1)  \\\n",
       "Unnamed: 0                                                                \n",
       "2023-07-01  8.030885  2.451122  10.695603  7.450570  1.269304  3.342631   \n",
       "2023-07-02  8.045755  2.528307  10.729535  7.467691  1.282147  3.227073   \n",
       "2023-07-03  7.845660  2.493652  10.709710  7.468844  1.273445  3.214212   \n",
       "2023-07-04  7.936348  2.479481  10.753400  7.424200  1.285398  3.169600   \n",
       "2023-07-05  8.017064  2.529456  10.939844  7.369914  1.300645  3.174790   \n",
       "2023-07-06  7.856343  2.530739  10.909184  7.362216  1.299923  3.133978   \n",
       "2023-07-07  8.271865  2.452341  10.770628  7.423415  1.301105  3.050977   \n",
       "\n",
       "              黄心菜(2)    黄白菜(2)  \n",
       "Unnamed: 0                      \n",
       "2023-07-01  3.572313  4.398471  \n",
       "2023-07-02  3.531505  4.456539  \n",
       "2023-07-03  3.444150  4.449769  \n",
       "2023-07-04  3.391892  4.500709  \n",
       "2023-07-05  3.274633  4.473733  \n",
       "2023-07-06  3.299617  4.509486  \n",
       "2023-07-07  3.204841  4.465784  \n",
       "\n",
       "[7 rows x 88 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#0读取数据 并 处理该小问异常值\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "cost = pd.read_csv('各年7.1-7.7单品批发价格.csv')\n",
    "sale_category = pd.read_csv('各年7.1-7.7各品类销量.csv')\n",
    "sale_product = pd.read_csv('各年7.1-7.7单品销量.csv')\n",
    "price = pd.read_csv('各年7.1-7.7单品售价.csv')\n",
    "price_discount = pd.read_csv('各年7.1-7.7单品折扣价格.csv')\n",
    "p1 = pd.read_csv('p1.csv')\n",
    "p2 = pd.read_csv('p2.csv')\n",
    "c_df = pd.read_csv('Prophet单品批发价格预测.csv')\n",
    "\n",
    "cost = cost.set_index(cost['日期'])\n",
    "cost = cost.drop('日期', axis=1)\n",
    "\n",
    "sale_category = sale_category.set_index(sale_category['销售日期'])\n",
    "sale_category = sale_category.drop('销售日期', axis=1)\n",
    "\n",
    "sale_product = sale_product.set_index(sale_product['销售日期'])\n",
    "sale_product = sale_product.drop('销售日期', axis=1)\n",
    "\n",
    "price = price.set_index(price['销售日期'])\n",
    "price = price.drop('销售日期', axis=1)\n",
    "\n",
    "price_discount = price_discount.set_index(price_discount['销售日期'])\n",
    "price_discount = price_discount.drop('销售日期', axis=1)\n",
    "\n",
    "c_df = c_df.set_index(c_df.iloc[:,0])\n",
    "c_df = c_df.drop(c_df.columns[0], axis=1)\n",
    "\n",
    "#处理cost中的异常值，作为基准处理price与sale_product中的异常值\n",
    "num_costna = cost.isna().sum()\n",
    "num_costna = num_costna[num_costna != 21]\n",
    "cost_processed = cost[num_costna.index]\n",
    "price_processed = price[cost_processed.columns]\n",
    "# price_discount_processed = price_discount[cost_processed.columns]\n",
    "c_df = c_df[cost_processed.columns]\n",
    "sale_product_processed = sale_product[cost_processed.columns]\n",
    "\n",
    "#处理sale_category中的异常值\n",
    "product_info = pd.read_excel('附件1.xlsx')\n",
    "info_dict = product_info[['单品名称', '分类名称']].set_index('单品名称').to_dict(orient='dict')['分类名称']\n",
    "\n",
    "zero = np.zeros((21,6))\n",
    "sale_category_processed =  pd.DataFrame(zero, columns=sale_category.columns, index=sale_category.index)\n",
    "for col in sale_product_processed.columns:\n",
    "    sale_category_processed[info_dict[col]] += sale_product_processed[col]\n",
    "\n",
    "c_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>水生根茎类</th>\n",
       "      <th>花叶类</th>\n",
       "      <th>花菜类</th>\n",
       "      <th>茄类</th>\n",
       "      <th>辣椒类</th>\n",
       "      <th>食用菌</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>销售日期</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-07-01</th>\n",
       "      <td>0.035020</td>\n",
       "      <td>0.649325</td>\n",
       "      <td>0.534132</td>\n",
       "      <td>0.548335</td>\n",
       "      <td>0.706815</td>\n",
       "      <td>0.438047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-02</th>\n",
       "      <td>0.030933</td>\n",
       "      <td>0.545266</td>\n",
       "      <td>0.458425</td>\n",
       "      <td>0.482229</td>\n",
       "      <td>0.560319</td>\n",
       "      <td>0.799339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-03</th>\n",
       "      <td>0.027826</td>\n",
       "      <td>0.593928</td>\n",
       "      <td>0.413310</td>\n",
       "      <td>0.559204</td>\n",
       "      <td>0.535410</td>\n",
       "      <td>0.634997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-04</th>\n",
       "      <td>0.044650</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.741609</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.763524</td>\n",
       "      <td>0.753677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-05</th>\n",
       "      <td>0.021434</td>\n",
       "      <td>0.877858</td>\n",
       "      <td>0.737066</td>\n",
       "      <td>0.871932</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.926712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-06</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.581589</td>\n",
       "      <td>0.418167</td>\n",
       "      <td>0.340544</td>\n",
       "      <td>0.458786</td>\n",
       "      <td>0.476180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-07</th>\n",
       "      <td>0.006687</td>\n",
       "      <td>0.388773</td>\n",
       "      <td>0.502066</td>\n",
       "      <td>0.480418</td>\n",
       "      <td>0.232267</td>\n",
       "      <td>0.788750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-01</th>\n",
       "      <td>0.094891</td>\n",
       "      <td>0.296490</td>\n",
       "      <td>0.211391</td>\n",
       "      <td>0.358950</td>\n",
       "      <td>0.402187</td>\n",
       "      <td>0.224630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-02</th>\n",
       "      <td>0.132134</td>\n",
       "      <td>0.169250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.406567</td>\n",
       "      <td>0.359975</td>\n",
       "      <td>0.277632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-03</th>\n",
       "      <td>0.440595</td>\n",
       "      <td>0.700123</td>\n",
       "      <td>0.568470</td>\n",
       "      <td>0.784755</td>\n",
       "      <td>0.663311</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-04</th>\n",
       "      <td>0.351606</td>\n",
       "      <td>0.695189</td>\n",
       "      <td>0.500375</td>\n",
       "      <td>0.628849</td>\n",
       "      <td>0.443670</td>\n",
       "      <td>0.527314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-05</th>\n",
       "      <td>0.200098</td>\n",
       "      <td>0.522977</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.844282</td>\n",
       "      <td>0.392697</td>\n",
       "      <td>0.388782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-06</th>\n",
       "      <td>0.084248</td>\n",
       "      <td>0.503240</td>\n",
       "      <td>0.481575</td>\n",
       "      <td>0.832143</td>\n",
       "      <td>0.291961</td>\n",
       "      <td>0.311540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-07</th>\n",
       "      <td>0.093125</td>\n",
       "      <td>0.253015</td>\n",
       "      <td>0.374937</td>\n",
       "      <td>0.472923</td>\n",
       "      <td>0.177677</td>\n",
       "      <td>0.148498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-01</th>\n",
       "      <td>0.202371</td>\n",
       "      <td>0.151862</td>\n",
       "      <td>0.078317</td>\n",
       "      <td>0.169210</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-02</th>\n",
       "      <td>0.424409</td>\n",
       "      <td>0.587710</td>\n",
       "      <td>0.636252</td>\n",
       "      <td>0.373274</td>\n",
       "      <td>0.421883</td>\n",
       "      <td>0.330986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-03</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.547738</td>\n",
       "      <td>0.565305</td>\n",
       "      <td>0.693956</td>\n",
       "      <td>0.458620</td>\n",
       "      <td>0.365030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-04</th>\n",
       "      <td>0.286291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.072083</td>\n",
       "      <td>0.182473</td>\n",
       "      <td>0.293185</td>\n",
       "      <td>0.152073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-05</th>\n",
       "      <td>0.519905</td>\n",
       "      <td>0.083823</td>\n",
       "      <td>0.426939</td>\n",
       "      <td>0.176685</td>\n",
       "      <td>0.335630</td>\n",
       "      <td>0.547816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-06</th>\n",
       "      <td>0.593640</td>\n",
       "      <td>0.241674</td>\n",
       "      <td>0.102192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.341008</td>\n",
       "      <td>0.167565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-07</th>\n",
       "      <td>0.659822</td>\n",
       "      <td>0.049734</td>\n",
       "      <td>0.628036</td>\n",
       "      <td>0.115847</td>\n",
       "      <td>0.200096</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               水生根茎类       花叶类       花菜类        茄类       辣椒类       食用菌\n",
       "销售日期                                                                  \n",
       "2020-07-01  0.035020  0.649325  0.534132  0.548335  0.706815  0.438047\n",
       "2020-07-02  0.030933  0.545266  0.458425  0.482229  0.560319  0.799339\n",
       "2020-07-03  0.027826  0.593928  0.413310  0.559204  0.535410  0.634997\n",
       "2020-07-04  0.044650  1.000000  0.741609  1.000000  0.763524  0.753677\n",
       "2020-07-05  0.021434  0.877858  0.737066  0.871932  1.000000  0.926712\n",
       "2020-07-06  0.000000  0.581589  0.418167  0.340544  0.458786  0.476180\n",
       "2020-07-07  0.006687  0.388773  0.502066  0.480418  0.232267  0.788750\n",
       "2021-07-01  0.094891  0.296490  0.211391  0.358950  0.402187  0.224630\n",
       "2021-07-02  0.132134  0.169250  0.000000  0.406567  0.359975  0.277632\n",
       "2021-07-03  0.440595  0.700123  0.568470  0.784755  0.663311  1.000000\n",
       "2021-07-04  0.351606  0.695189  0.500375  0.628849  0.443670  0.527314\n",
       "2021-07-05  0.200098  0.522977  1.000000  0.844282  0.392697  0.388782\n",
       "2021-07-06  0.084248  0.503240  0.481575  0.832143  0.291961  0.311540\n",
       "2021-07-07  0.093125  0.253015  0.374937  0.472923  0.177677  0.148498\n",
       "2022-07-01  0.202371  0.151862  0.078317  0.169210  0.000000  0.089294\n",
       "2022-07-02  0.424409  0.587710  0.636252  0.373274  0.421883  0.330986\n",
       "2022-07-03  1.000000  0.547738  0.565305  0.693956  0.458620  0.365030\n",
       "2022-07-04  0.286291  0.000000  0.072083  0.182473  0.293185  0.152073\n",
       "2022-07-05  0.519905  0.083823  0.426939  0.176685  0.335630  0.547816\n",
       "2022-07-06  0.593640  0.241674  0.102192  0.000000  0.341008  0.167565\n",
       "2022-07-07  0.659822  0.049734  0.628036  0.115847  0.200096  0.000000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.计算S_gt\n",
    "\n",
    "S_max = sale_category_processed.apply(lambda x:x.max())\n",
    "S_min = sale_category_processed.apply(lambda x:x.min())\n",
    "\n",
    "S_gt = (sale_category_processed - S_min) / (S_max - S_min)\n",
    "S_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>水生根茎类</th>\n",
       "      <th>花叶类</th>\n",
       "      <th>花菜类</th>\n",
       "      <th>茄类</th>\n",
       "      <th>辣椒类</th>\n",
       "      <th>食用菌</th>\n",
       "      <th>date_factor</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>销售日期</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-07-01</th>\n",
       "      <td>0.706978</td>\n",
       "      <td>0.590086</td>\n",
       "      <td>0.510566</td>\n",
       "      <td>0.685442</td>\n",
       "      <td>0.820073</td>\n",
       "      <td>0.576699</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-02</th>\n",
       "      <td>0.710128</td>\n",
       "      <td>0.757272</td>\n",
       "      <td>0.591105</td>\n",
       "      <td>1.355157</td>\n",
       "      <td>0.744661</td>\n",
       "      <td>0.650298</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-03</th>\n",
       "      <td>0.782531</td>\n",
       "      <td>0.699971</td>\n",
       "      <td>0.598779</td>\n",
       "      <td>0.902886</td>\n",
       "      <td>0.623897</td>\n",
       "      <td>0.729150</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-04</th>\n",
       "      <td>0.688605</td>\n",
       "      <td>0.648614</td>\n",
       "      <td>0.553701</td>\n",
       "      <td>0.879172</td>\n",
       "      <td>0.737760</td>\n",
       "      <td>0.685781</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-05</th>\n",
       "      <td>0.845018</td>\n",
       "      <td>0.670352</td>\n",
       "      <td>0.512468</td>\n",
       "      <td>0.799566</td>\n",
       "      <td>0.670179</td>\n",
       "      <td>0.675019</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-06</th>\n",
       "      <td>0.834862</td>\n",
       "      <td>0.641013</td>\n",
       "      <td>0.533176</td>\n",
       "      <td>0.891419</td>\n",
       "      <td>0.729580</td>\n",
       "      <td>0.673781</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-07</th>\n",
       "      <td>0.831502</td>\n",
       "      <td>0.667985</td>\n",
       "      <td>0.590915</td>\n",
       "      <td>0.829822</td>\n",
       "      <td>0.834436</td>\n",
       "      <td>0.728714</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-01</th>\n",
       "      <td>0.683266</td>\n",
       "      <td>0.754930</td>\n",
       "      <td>0.548351</td>\n",
       "      <td>0.814191</td>\n",
       "      <td>0.880824</td>\n",
       "      <td>0.621025</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-02</th>\n",
       "      <td>0.719280</td>\n",
       "      <td>0.776297</td>\n",
       "      <td>0.701186</td>\n",
       "      <td>0.758584</td>\n",
       "      <td>1.059710</td>\n",
       "      <td>0.617144</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-03</th>\n",
       "      <td>0.758698</td>\n",
       "      <td>0.810162</td>\n",
       "      <td>0.656476</td>\n",
       "      <td>0.663966</td>\n",
       "      <td>0.665037</td>\n",
       "      <td>0.596233</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-04</th>\n",
       "      <td>0.821925</td>\n",
       "      <td>0.764080</td>\n",
       "      <td>0.718606</td>\n",
       "      <td>0.747862</td>\n",
       "      <td>0.671431</td>\n",
       "      <td>0.583546</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-05</th>\n",
       "      <td>0.838986</td>\n",
       "      <td>0.792611</td>\n",
       "      <td>0.739622</td>\n",
       "      <td>0.856015</td>\n",
       "      <td>0.656669</td>\n",
       "      <td>0.548713</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-06</th>\n",
       "      <td>0.856445</td>\n",
       "      <td>0.775495</td>\n",
       "      <td>0.756460</td>\n",
       "      <td>0.833522</td>\n",
       "      <td>1.042306</td>\n",
       "      <td>0.646597</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-07</th>\n",
       "      <td>0.664574</td>\n",
       "      <td>0.715576</td>\n",
       "      <td>0.740733</td>\n",
       "      <td>0.573811</td>\n",
       "      <td>0.671492</td>\n",
       "      <td>0.624336</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-01</th>\n",
       "      <td>0.533035</td>\n",
       "      <td>0.701356</td>\n",
       "      <td>0.511775</td>\n",
       "      <td>0.726940</td>\n",
       "      <td>0.397740</td>\n",
       "      <td>0.600465</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-02</th>\n",
       "      <td>0.580795</td>\n",
       "      <td>0.799917</td>\n",
       "      <td>0.567729</td>\n",
       "      <td>0.590599</td>\n",
       "      <td>0.591665</td>\n",
       "      <td>0.634730</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-03</th>\n",
       "      <td>0.885647</td>\n",
       "      <td>0.798314</td>\n",
       "      <td>0.597720</td>\n",
       "      <td>0.752330</td>\n",
       "      <td>0.461913</td>\n",
       "      <td>0.596649</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-04</th>\n",
       "      <td>0.527175</td>\n",
       "      <td>0.827112</td>\n",
       "      <td>0.594896</td>\n",
       "      <td>0.938876</td>\n",
       "      <td>0.666269</td>\n",
       "      <td>0.560415</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-05</th>\n",
       "      <td>0.529928</td>\n",
       "      <td>0.740338</td>\n",
       "      <td>0.612473</td>\n",
       "      <td>0.808129</td>\n",
       "      <td>0.578639</td>\n",
       "      <td>0.490737</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-06</th>\n",
       "      <td>0.486617</td>\n",
       "      <td>0.788330</td>\n",
       "      <td>0.601705</td>\n",
       "      <td>0.968031</td>\n",
       "      <td>0.593573</td>\n",
       "      <td>0.528860</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-07</th>\n",
       "      <td>0.505096</td>\n",
       "      <td>0.850991</td>\n",
       "      <td>0.459541</td>\n",
       "      <td>0.849324</td>\n",
       "      <td>0.571314</td>\n",
       "      <td>0.547676</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               水生根茎类       花叶类       花菜类        茄类       辣椒类       食用菌  \\\n",
       "销售日期                                                                     \n",
       "2020-07-01  0.706978  0.590086  0.510566  0.685442  0.820073  0.576699   \n",
       "2020-07-02  0.710128  0.757272  0.591105  1.355157  0.744661  0.650298   \n",
       "2020-07-03  0.782531  0.699971  0.598779  0.902886  0.623897  0.729150   \n",
       "2020-07-04  0.688605  0.648614  0.553701  0.879172  0.737760  0.685781   \n",
       "2020-07-05  0.845018  0.670352  0.512468  0.799566  0.670179  0.675019   \n",
       "2020-07-06  0.834862  0.641013  0.533176  0.891419  0.729580  0.673781   \n",
       "2020-07-07  0.831502  0.667985  0.590915  0.829822  0.834436  0.728714   \n",
       "2021-07-01  0.683266  0.754930  0.548351  0.814191  0.880824  0.621025   \n",
       "2021-07-02  0.719280  0.776297  0.701186  0.758584  1.059710  0.617144   \n",
       "2021-07-03  0.758698  0.810162  0.656476  0.663966  0.665037  0.596233   \n",
       "2021-07-04  0.821925  0.764080  0.718606  0.747862  0.671431  0.583546   \n",
       "2021-07-05  0.838986  0.792611  0.739622  0.856015  0.656669  0.548713   \n",
       "2021-07-06  0.856445  0.775495  0.756460  0.833522  1.042306  0.646597   \n",
       "2021-07-07  0.664574  0.715576  0.740733  0.573811  0.671492  0.624336   \n",
       "2022-07-01  0.533035  0.701356  0.511775  0.726940  0.397740  0.600465   \n",
       "2022-07-02  0.580795  0.799917  0.567729  0.590599  0.591665  0.634730   \n",
       "2022-07-03  0.885647  0.798314  0.597720  0.752330  0.461913  0.596649   \n",
       "2022-07-04  0.527175  0.827112  0.594896  0.938876  0.666269  0.560415   \n",
       "2022-07-05  0.529928  0.740338  0.612473  0.808129  0.578639  0.490737   \n",
       "2022-07-06  0.486617  0.788330  0.601705  0.968031  0.593573  0.528860   \n",
       "2022-07-07  0.505096  0.850991  0.459541  0.849324  0.571314  0.547676   \n",
       "\n",
       "            date_factor  \n",
       "销售日期                     \n",
       "2020-07-01     0.000000  \n",
       "2020-07-02     0.166667  \n",
       "2020-07-03     0.333333  \n",
       "2020-07-04     0.500000  \n",
       "2020-07-05     0.666667  \n",
       "2020-07-06     0.833333  \n",
       "2020-07-07     1.000000  \n",
       "2021-07-01     0.000000  \n",
       "2021-07-02     0.166667  \n",
       "2021-07-03     0.333333  \n",
       "2021-07-04     0.500000  \n",
       "2021-07-05     0.666667  \n",
       "2021-07-06     0.833333  \n",
       "2021-07-07     1.000000  \n",
       "2022-07-01     0.000000  \n",
       "2022-07-02     0.166667  \n",
       "2022-07-03     0.333333  \n",
       "2022-07-04     0.500000  \n",
       "2022-07-05     0.666667  \n",
       "2022-07-06     0.833333  \n",
       "2022-07-07     1.000000  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2.计算w\n",
    "import datetime\n",
    "w_k = price_processed / cost_processed - 1\n",
    "# 0不重要，对应的当日单品销量会为0\n",
    "w_k = w_k.fillna(0)\n",
    "w_prime = w_k * sale_product_processed\n",
    "\n",
    "zero = np.zeros((21,6))\n",
    "w_x_Θ =  pd.DataFrame(zero, columns=sale_category.columns, index=sale_category.index)\n",
    "for col in w_prime.columns:\n",
    "    w_x_Θ[info_dict[col]] += w_prime[col]\n",
    "\n",
    "w = w_x_Θ / sale_category_processed\n",
    "\n",
    "t_list = []\n",
    "for date in w.index:\n",
    "    time = datetime.datetime.strptime(date, '%Y-%m-%d')\n",
    "    t_list.append(time.day)\n",
    "t_np = np.array(t_list)\n",
    "t_norm = (t_np - t_np.min()) / (t_np.max() - t_np.min())\n",
    "w['date_factor'] = t_norm\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.模型拟合预测S_j\n",
    "import numpy as np\n",
    "import math\n",
    "class MyLogisticRegression:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Class contructor.\n",
    "        \n",
    "        :attr beta: weights vector of logistic regression.\n",
    "        \"\"\"\n",
    "        self.beta = None\n",
    "\n",
    "    def fx(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the value of f(x) given x.\n",
    "\n",
    "        :param X: numpy.ndarray with a shape of (n, d), input data.\n",
    "        :return:\n",
    "            fx_value: numpy.ndarray with a length of n, output of f(x).\n",
    "        \"\"\"\n",
    "        n = X.shape[0]\n",
    "        fx_value = np.zeros(n)\n",
    "        for i in range(n):\n",
    "            data = self.beta@X[i].T\n",
    "            fx_value[i] = data\n",
    "        fx_value = 1/(1+np.exp(-fx_value))\n",
    "        return fx_value\n",
    "\n",
    "    def loss(self, fx_value, y):\n",
    "        \"\"\"\n",
    "        Calculate the loss function given the calculated value f(x) and the true label y.\n",
    "\n",
    "        :param fx_value: numpy.ndarray with a length of n, \n",
    "                         a vector of hypothesis function values on these samples, which is the output of the function fx\n",
    "        :param y: numpy.ndarray with a length of n, \n",
    "                  a vector of the true labels of these samples\n",
    "        :return:\n",
    "            CELoss: a float value of the cross-entropy loss.\n",
    "        \"\"\"\n",
    "        loss = 0\n",
    "        n = fx_value.shape[0]\n",
    "        for i in range(n):\n",
    "            loss = loss + (-y[i])*math.log(fx_value[i])-(1-y[i])*math.log((1-fx_value[i]))\n",
    "        loss = loss / n\n",
    "        return loss\n",
    "\n",
    "    def fit(self, X, y, n_iters = 10000, alpha = 0.01):\n",
    "        \"\"\"\n",
    "        Train the model using gradient descent\n",
    "\n",
    "        :param X: numpy.ndarray with a shape of (n*d), input data.\n",
    "        :param y: numpy.ndarray with a length of n, the true labels of these samples\n",
    "        :param n_iters: int, number of iterations\n",
    "        :param alpha: float, learning rate\n",
    "        :return:\n",
    "            CELoss_list: list with a length of n_iters+1, \n",
    "                         contains the loss values before the gradient descent and after the gradient descent.\n",
    "        \"\"\"\n",
    "\n",
    "        n, d = X.shape\n",
    "        \n",
    "        self.beta = np.zeros(d+1)\n",
    "        #the first element in X_ is 1\n",
    "        X_ = np.column_stack([np.ones(n), X])\n",
    "        CELoss_list = [self.loss(self.fx(X_), y)]\n",
    "        \n",
    "        for i in range(n_iters):\n",
    "            fx_value = self.fx(X_)\n",
    "            offset = np.zeros(d+1)\n",
    "            for j in range(n):\n",
    "                offset = offset + (-y[j]/fx_value[j]+(1-y[j])/(1-fx_value[j]))*fx_value[j]*(1-fx_value[j])*X_[j]\n",
    "            offset = offset / n\n",
    "            self.beta = self.beta - offset #*alpha\n",
    "            \n",
    "            CELoss_list.append(self.loss(self.fx(X_), y))\n",
    "        \n",
    "        return CELoss_list\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the labels of input instances.\n",
    "\n",
    "        :param X: numpy.ndarray with a shape of (n*d), input data.\n",
    "        :return:\n",
    "            y_hat: numpy.ndarray with a length of n, the predicted labels of these samples\n",
    "        \"\"\"\n",
    "        n = X.shape[0]\n",
    "        X = np.column_stack([np.ones(n), X])\n",
    "        fx_value = self.fx(X)\n",
    "        return fx_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Cross-Entropy Loss: 0.6931471805599453\n",
      "Step 10: Cross-Entropy Loss: 0.5595020085029164\n",
      "Step 20: Cross-Entropy Loss: 0.557970407623832\n",
      "Step 30: Cross-Entropy Loss: 0.556815537655738\n",
      "Step 40: Cross-Entropy Loss: 0.555913510586324\n",
      "Step 50: Cross-Entropy Loss: 0.5551921547967171\n",
      "Step 60: Cross-Entropy Loss: 0.5546009068431891\n",
      "Step 70: Cross-Entropy Loss: 0.5541042926904609\n",
      "Step 80: Cross-Entropy Loss: 0.5536773725596363\n",
      "Step 90: Cross-Entropy Loss: 0.5533025634453623\n",
      "Step 100: Cross-Entropy Loss: 0.5529674244627599\n",
      "Step 110: Cross-Entropy Loss: 0.5526631130905328\n",
      "Step 120: Cross-Entropy Loss: 0.5523833083749448\n",
      "Step 130: Cross-Entropy Loss: 0.5521234591514348\n",
      "Step 140: Cross-Entropy Loss: 0.55188025863104\n",
      "Step 150: Cross-Entropy Loss: 0.5516512767944884\n",
      "Step 160: Cross-Entropy Loss: 0.5514347029206337\n",
      "Step 170: Cross-Entropy Loss: 0.5512291650633576\n",
      "Step 180: Cross-Entropy Loss: 0.5510336033477005\n",
      "Step 190: Cross-Entropy Loss: 0.5508471809448459\n",
      "Step 200: Cross-Entropy Loss: 0.5506692214491783\n",
      "Step 210: Cross-Entropy Loss: 0.5504991647700037\n",
      "Step 220: Cross-Entropy Loss: 0.5503365360158645\n",
      "Step 230: Cross-Entropy Loss: 0.5501809235020896\n",
      "Step 240: Cross-Entropy Loss: 0.5500319631683899\n",
      "Step 250: Cross-Entropy Loss: 0.5498893275028562\n",
      "Step 260: Cross-Entropy Loss: 0.5497527176360706\n",
      "Step 270: Cross-Entropy Loss: 0.5496218576669069\n",
      "Step 280: Cross-Entropy Loss: 0.5494964905607981\n",
      "Step 290: Cross-Entropy Loss: 0.54937637515722\n",
      "Step 300: Cross-Entropy Loss: 0.5492612839607941\n",
      "Step 310: Cross-Entropy Loss: 0.5491510014870846\n",
      "Step 320: Cross-Entropy Loss: 0.5490453230020952\n",
      "Step 330: Cross-Entropy Loss: 0.5489440535422055\n",
      "Step 340: Cross-Entropy Loss: 0.5488470071348305\n",
      "Step 350: Cross-Entropy Loss: 0.5487540061636573\n",
      "Step 360: Cross-Entropy Loss: 0.5486648808388949\n",
      "Step 370: Cross-Entropy Loss: 0.5485794687446065\n",
      "Step 380: Cross-Entropy Loss: 0.5484976144433918\n",
      "Step 390: Cross-Entropy Loss: 0.5484191691244369\n",
      "Step 400: Cross-Entropy Loss: 0.5483439902849987\n",
      "Step 410: Cross-Entropy Loss: 0.5482719414382368\n",
      "Step 420: Cross-Entropy Loss: 0.5482028918423097\n",
      "Step 430: Cross-Entropy Loss: 0.5481367162470596\n",
      "Step 440: Cross-Entropy Loss: 0.5480732946556068\n",
      "Step 450: Cross-Entropy Loss: 0.5480125120988727\n",
      "Step 460: Cross-Entropy Loss: 0.5479542584215485\n",
      "Step 470: Cross-Entropy Loss: 0.5478984280783766\n",
      "Step 480: Cross-Entropy Loss: 0.5478449199398557\n",
      "Step 490: Cross-Entropy Loss: 0.5477936371066726\n",
      "Step 500: Cross-Entropy Loss: 0.547744486732275\n",
      "Step 510: Cross-Entropy Loss: 0.5476973798531029\n",
      "Step 520: Cross-Entropy Loss: 0.5476522312260604\n",
      "Step 530: Cross-Entropy Loss: 0.5476089591728641\n",
      "Step 540: Cross-Entropy Loss: 0.5475674854309324\n",
      "Step 550: Cross-Entropy Loss: 0.547527735010522\n",
      "Step 560: Cross-Entropy Loss: 0.5474896360578321\n",
      "Step 570: Cross-Entropy Loss: 0.5474531197238169\n",
      "Step 580: Cross-Entropy Loss: 0.5474181200384682\n",
      "Step 590: Cross-Entropy Loss: 0.5473845737903316\n",
      "Step 600: Cross-Entropy Loss: 0.5473524204110445\n",
      "Step 610: Cross-Entropy Loss: 0.5473216018646847\n",
      "Step 620: Cross-Entropy Loss: 0.5472920625417336\n",
      "Step 630: Cross-Entropy Loss: 0.5472637491574581\n",
      "Step 640: Cross-Entropy Loss: 0.5472366106545387\n",
      "Step 650: Cross-Entropy Loss: 0.5472105981097583\n",
      "Step 660: Cross-Entropy Loss: 0.5471856646445934\n",
      "Step 670: Cross-Entropy Loss: 0.5471617653395422\n",
      "Step 680: Cross-Entropy Loss: 0.5471388571520353\n",
      "Step 690: Cross-Entropy Loss: 0.5471168988377821\n",
      "Step 700: Cross-Entropy Loss: 0.5470958508754131\n",
      "Step 710: Cross-Entropy Loss: 0.5470756753942746\n",
      "Step 720: Cross-Entropy Loss: 0.5470563361052537\n",
      "Step 730: Cross-Entropy Loss: 0.5470377982345005\n",
      "Step 740: Cross-Entropy Loss: 0.5470200284599311\n",
      "Step 750: Cross-Entropy Loss: 0.5470029948503945\n",
      "Step 760: Cross-Entropy Loss: 0.5469866668073922\n",
      "Step 770: Cross-Entropy Loss: 0.5469710150092438\n",
      "Step 780: Cross-Entropy Loss: 0.5469560113575964\n",
      "Step 790: Cross-Entropy Loss: 0.5469416289261805\n",
      "Step 800: Cross-Entropy Loss: 0.5469278419117187\n",
      "Step 810: Cross-Entropy Loss: 0.5469146255868942\n",
      "Step 820: Cross-Entropy Loss: 0.5469019562552969\n",
      "Step 830: Cross-Entropy Loss: 0.5468898112082611\n",
      "Step 840: Cross-Entropy Loss: 0.5468781686835165\n",
      "Step 850: Cross-Entropy Loss: 0.5468670078255764\n",
      "Step 860: Cross-Entropy Loss: 0.5468563086477909\n",
      "Step 870: Cross-Entropy Loss: 0.5468460519959917\n",
      "Step 880: Cross-Entropy Loss: 0.5468362195136679\n",
      "Step 890: Cross-Entropy Loss: 0.546826793608601\n",
      "Step 900: Cross-Entropy Loss: 0.5468177574209031\n",
      "Step 910: Cross-Entropy Loss: 0.5468090947923985\n",
      "Step 920: Cross-Entropy Loss: 0.5468007902372884\n",
      "Step 930: Cross-Entropy Loss: 0.5467928289140498\n",
      "Step 940: Cross-Entropy Loss: 0.5467851965985137\n",
      "Step 950: Cross-Entropy Loss: 0.546777879658072\n",
      "Step 960: Cross-Entropy Loss: 0.5467708650269698\n",
      "Step 970: Cross-Entropy Loss: 0.5467641401826316\n",
      "Step 980: Cross-Entropy Loss: 0.5467576931229832\n",
      "Step 990: Cross-Entropy Loss: 0.5467515123447242\n",
      "Step 1000: Cross-Entropy Loss: 0.546745586822511\n",
      "Step 1010: Cross-Entropy Loss: 0.5467399059890146\n",
      "Step 1020: Cross-Entropy Loss: 0.546734459715813\n",
      "Step 1030: Cross-Entropy Loss: 0.5467292382950854\n",
      "Step 1040: Cross-Entropy Loss: 0.5467242324220737\n",
      "Step 1050: Cross-Entropy Loss: 0.5467194331782784\n",
      "Step 1060: Cross-Entropy Loss: 0.546714832015359\n",
      "Step 1070: Cross-Entropy Loss: 0.546710420739708\n",
      "Step 1080: Cross-Entropy Loss: 0.546706191497673\n",
      "Step 1090: Cross-Entropy Loss: 0.5467021367613942\n",
      "Step 1100: Cross-Entropy Loss: 0.5466982493152368\n",
      "Step 1110: Cross-Entropy Loss: 0.5466945222427922\n",
      "Step 1120: Cross-Entropy Loss: 0.5466909489144198\n",
      "Step 1130: Cross-Entropy Loss: 0.5466875229753135\n",
      "Step 1140: Cross-Entropy Loss: 0.546684238334064\n",
      "Step 1150: Cross-Entropy Loss: 0.5466810891517019\n",
      "Step 1160: Cross-Entropy Loss: 0.5466780698311973\n",
      "Step 1170: Cross-Entropy Loss: 0.5466751750073989\n",
      "Step 1180: Cross-Entropy Loss: 0.5466723995373926\n",
      "Step 1190: Cross-Entropy Loss: 0.5466697384912637\n",
      "Step 1200: Cross-Entropy Loss: 0.5466671871432445\n",
      "Step 1210: Cross-Entropy Loss: 0.5466647409632316\n",
      "Step 1220: Cross-Entropy Loss: 0.5466623956086569\n",
      "Step 1230: Cross-Entropy Loss: 0.5466601469166981\n",
      "Step 1240: Cross-Entropy Loss: 0.5466579908968139\n",
      "Step 1250: Cross-Entropy Loss: 0.5466559237235922\n",
      "Step 1260: Cross-Entropy Loss: 0.5466539417298921\n",
      "Step 1270: Cross-Entropy Loss: 0.5466520414002785\n",
      "Step 1280: Cross-Entropy Loss: 0.5466502193647239\n",
      "Step 1290: Cross-Entropy Loss: 0.5466484723925765\n",
      "Step 1300: Cross-Entropy Loss: 0.5466467973867786\n",
      "Step 1310: Cross-Entropy Loss: 0.5466451913783248\n",
      "Step 1320: Cross-Entropy Loss: 0.5466436515209538\n",
      "Step 1330: Cross-Entropy Loss: 0.5466421750860571\n",
      "Step 1340: Cross-Entropy Loss: 0.5466407594578024\n",
      "Step 1350: Cross-Entropy Loss: 0.5466394021284615\n",
      "Step 1360: Cross-Entropy Loss: 0.5466381006939264\n",
      "Step 1370: Cross-Entropy Loss: 0.5466368528494199\n",
      "Step 1380: Cross-Entropy Loss: 0.5466356563853777\n",
      "Step 1390: Cross-Entropy Loss: 0.5466345091835064\n",
      "Step 1400: Cross-Entropy Loss: 0.5466334092130033\n",
      "Step 1410: Cross-Entropy Loss: 0.5466323545269327\n",
      "Step 1420: Cross-Entropy Loss: 0.5466313432587548\n",
      "Step 1430: Cross-Entropy Loss: 0.5466303736189972\n",
      "Step 1440: Cross-Entropy Loss: 0.5466294438920645\n",
      "Step 1450: Cross-Entropy Loss: 0.5466285524331831\n",
      "Step 1460: Cross-Entropy Loss: 0.5466276976654678\n",
      "Step 1470: Cross-Entropy Loss: 0.5466268780771164\n",
      "Step 1480: Cross-Entropy Loss: 0.5466260922187148\n",
      "Step 1490: Cross-Entropy Loss: 0.5466253387006581\n",
      "Step 1500: Cross-Entropy Loss: 0.5466246161906765\n",
      "Step 1510: Cross-Entropy Loss: 0.5466239234114655\n",
      "Step 1520: Cross-Entropy Loss: 0.5466232591384124\n",
      "Step 1530: Cross-Entropy Loss: 0.5466226221974188\n",
      "Step 1540: Cross-Entropy Loss: 0.5466220114628116\n",
      "Step 1550: Cross-Entropy Loss: 0.546621425855344\n",
      "Step 1560: Cross-Entropy Loss: 0.5466208643402741\n",
      "Step 1570: Cross-Entropy Loss: 0.546620325925528\n",
      "Step 1580: Cross-Entropy Loss: 0.5466198096599365\n",
      "Step 1590: Cross-Entropy Loss: 0.5466193146315457\n",
      "Step 1600: Cross-Entropy Loss: 0.5466188399659961\n",
      "Step 1610: Cross-Entropy Loss: 0.5466183848249719\n",
      "Step 1620: Cross-Entropy Loss: 0.546617948404711\n",
      "Step 1630: Cross-Entropy Loss: 0.5466175299345781\n",
      "Step 1640: Cross-Entropy Loss: 0.5466171286756993\n",
      "Step 1650: Cross-Entropy Loss: 0.5466167439196478\n",
      "Step 1660: Cross-Entropy Loss: 0.5466163749871898\n",
      "Step 1670: Cross-Entropy Loss: 0.5466160212270779\n",
      "Step 1680: Cross-Entropy Loss: 0.5466156820148973\n",
      "Step 1690: Cross-Entropy Loss: 0.546615356751957\n",
      "Step 1700: Cross-Entropy Loss: 0.5466150448642307\n",
      "Step 1710: Cross-Entropy Loss: 0.5466147458013371\n",
      "Step 1720: Cross-Entropy Loss: 0.5466144590355654\n",
      "Step 1730: Cross-Entropy Loss: 0.5466141840609399\n",
      "Step 1740: Cross-Entropy Loss: 0.5466139203923239\n",
      "Step 1750: Cross-Entropy Loss: 0.5466136675645602\n",
      "Step 1760: Cross-Entropy Loss: 0.5466134251316461\n",
      "Step 1770: Cross-Entropy Loss: 0.5466131926659457\n",
      "Step 1780: Cross-Entropy Loss: 0.5466129697574299\n",
      "Step 1790: Cross-Entropy Loss: 0.5466127560129527\n",
      "Step 1800: Cross-Entropy Loss: 0.5466125510555547\n",
      "Step 1810: Cross-Entropy Loss: 0.5466123545237946\n",
      "Step 1820: Cross-Entropy Loss: 0.5466121660711115\n",
      "Step 1830: Cross-Entropy Loss: 0.5466119853652097\n",
      "Step 1840: Cross-Entropy Loss: 0.5466118120874726\n",
      "Step 1850: Cross-Entropy Loss: 0.5466116459323971\n",
      "Step 1860: Cross-Entropy Loss: 0.5466114866070549\n",
      "Step 1870: Cross-Entropy Loss: 0.5466113338305739\n",
      "Step 1880: Cross-Entropy Loss: 0.5466111873336406\n",
      "Step 1890: Cross-Entropy Loss: 0.5466110468580246\n",
      "Step 1900: Cross-Entropy Loss: 0.5466109121561222\n",
      "Step 1910: Cross-Entropy Loss: 0.5466107829905186\n",
      "Step 1920: Cross-Entropy Loss: 0.5466106591335678\n",
      "Step 1930: Cross-Entropy Loss: 0.5466105403669902\n",
      "Step 1940: Cross-Entropy Loss: 0.5466104264814882\n",
      "Step 1950: Cross-Entropy Loss: 0.5466103172763734\n",
      "Step 1960: Cross-Entropy Loss: 0.5466102125592163\n",
      "Step 1970: Cross-Entropy Loss: 0.5466101121455013\n",
      "Step 1980: Cross-Entropy Loss: 0.546610015858305\n",
      "Step 1990: Cross-Entropy Loss: 0.5466099235279815\n",
      "Step 2000: Cross-Entropy Loss: 0.5466098349918636\n",
      "Step 2010: Cross-Entropy Loss: 0.5466097500939737\n",
      "Step 2020: Cross-Entropy Loss: 0.5466096686847516\n",
      "Step 2030: Cross-Entropy Loss: 0.5466095906207875\n",
      "Step 2040: Cross-Entropy Loss: 0.5466095157645705\n",
      "Step 2050: Cross-Entropy Loss: 0.5466094439842446\n",
      "Step 2060: Cross-Entropy Loss: 0.5466093751533767\n",
      "Step 2070: Cross-Entropy Loss: 0.5466093091507337\n",
      "Step 2080: Cross-Entropy Loss: 0.5466092458600674\n",
      "Step 2090: Cross-Entropy Loss: 0.5466091851699105\n",
      "Step 2100: Cross-Entropy Loss: 0.5466091269733792\n",
      "Step 2110: Cross-Entropy Loss: 0.5466090711679843\n",
      "Step 2120: Cross-Entropy Loss: 0.5466090176554517\n",
      "Step 2130: Cross-Entropy Loss: 0.5466089663415475\n",
      "Step 2140: Cross-Entropy Loss: 0.5466089171359124\n",
      "Step 2150: Cross-Entropy Loss: 0.5466088699519026\n",
      "Step 2160: Cross-Entropy Loss: 0.5466088247064358\n",
      "Step 2170: Cross-Entropy Loss: 0.5466087813198466\n",
      "Step 2180: Cross-Entropy Loss: 0.5466087397157439\n",
      "Step 2190: Cross-Entropy Loss: 0.5466086998208775\n",
      "Step 2200: Cross-Entropy Loss: 0.5466086615650086\n",
      "Step 2210: Cross-Entropy Loss: 0.5466086248807855\n",
      "Step 2220: Cross-Entropy Loss: 0.5466085897036258\n",
      "Step 2230: Cross-Entropy Loss: 0.5466085559716011\n",
      "Step 2240: Cross-Entropy Loss: 0.5466085236253292\n",
      "Step 2250: Cross-Entropy Loss: 0.5466084926078688\n",
      "Step 2260: Cross-Entropy Loss: 0.5466084628646188\n",
      "Step 2270: Cross-Entropy Loss: 0.546608434343223\n",
      "Step 2280: Cross-Entropy Loss: 0.5466084069934765\n",
      "Step 2290: Cross-Entropy Loss: 0.5466083807672384\n",
      "Step 2300: Cross-Entropy Loss: 0.546608355618346\n",
      "Step 2310: Cross-Entropy Loss: 0.5466083315025344\n",
      "Step 2320: Cross-Entropy Loss: 0.5466083083773577\n",
      "Step 2330: Cross-Entropy Loss: 0.5466082862021145\n",
      "Step 2340: Cross-Entropy Loss: 0.5466082649377764\n",
      "Step 2350: Cross-Entropy Loss: 0.546608244546918\n",
      "Step 2360: Cross-Entropy Loss: 0.5466082249936532\n",
      "Step 2370: Cross-Entropy Loss: 0.54660820624357\n",
      "Step 2380: Cross-Entropy Loss: 0.5466081882636707\n",
      "Step 2390: Cross-Entropy Loss: 0.5466081710223136\n",
      "Step 2400: Cross-Entropy Loss: 0.5466081544891571\n",
      "Step 2410: Cross-Entropy Loss: 0.5466081386351069\n",
      "Step 2420: Cross-Entropy Loss: 0.5466081234322637\n",
      "Step 2430: Cross-Entropy Loss: 0.546608108853875\n",
      "Step 2440: Cross-Entropy Loss: 0.5466080948742874\n",
      "Step 2450: Cross-Entropy Loss: 0.5466080814689016\n",
      "Step 2460: Cross-Entropy Loss: 0.5466080686141295\n",
      "Step 2470: Cross-Entropy Loss: 0.5466080562873517\n",
      "Step 2480: Cross-Entropy Loss: 0.5466080444668784\n",
      "Step 2490: Cross-Entropy Loss: 0.5466080331319109\n",
      "Step 2500: Cross-Entropy Loss: 0.5466080222625053\n",
      "Step 2510: Cross-Entropy Loss: 0.546608011839537\n",
      "Step 2520: Cross-Entropy Loss: 0.5466080018446668\n",
      "Step 2530: Cross-Entropy Loss: 0.5466079922603101\n",
      "Step 2540: Cross-Entropy Loss: 0.5466079830696031\n",
      "Step 2550: Cross-Entropy Loss: 0.5466079742563769\n",
      "Step 2560: Cross-Entropy Loss: 0.5466079658051246\n",
      "Step 2570: Cross-Entropy Loss: 0.5466079577009781\n",
      "Step 2580: Cross-Entropy Loss: 0.5466079499296799\n",
      "Step 2590: Cross-Entropy Loss: 0.5466079424775574\n",
      "Step 2600: Cross-Entropy Loss: 0.5466079353315005\n",
      "Step 2610: Cross-Entropy Loss: 0.5466079284789371\n",
      "Step 2620: Cross-Entropy Loss: 0.5466079219078127\n",
      "Step 2630: Cross-Entropy Loss: 0.5466079156065666\n",
      "Step 2640: Cross-Entropy Loss: 0.5466079095641139\n",
      "Step 2650: Cross-Entropy Loss: 0.5466079037698248\n",
      "Step 2660: Cross-Entropy Loss: 0.5466078982135062\n",
      "Step 2670: Cross-Entropy Loss: 0.5466078928853839\n",
      "Step 2680: Cross-Entropy Loss: 0.5466078877760849\n",
      "Step 2690: Cross-Entropy Loss: 0.5466078828766217\n",
      "Step 2700: Cross-Entropy Loss: 0.5466078781783749\n",
      "Step 2710: Cross-Entropy Loss: 0.546607873673081\n",
      "Step 2720: Cross-Entropy Loss: 0.5466078693528145\n",
      "Step 2730: Cross-Entropy Loss: 0.5466078652099758\n",
      "Step 2740: Cross-Entropy Loss: 0.5466078612372774\n",
      "Step 2750: Cross-Entropy Loss: 0.5466078574277318\n",
      "Step 2760: Cross-Entropy Loss: 0.5466078537746378\n",
      "Step 2770: Cross-Entropy Loss: 0.5466078502715698\n",
      "Step 2780: Cross-Entropy Loss: 0.5466078469123654\n",
      "Step 2790: Cross-Entropy Loss: 0.546607843691117\n",
      "Step 2800: Cross-Entropy Loss: 0.5466078406021582\n",
      "Step 2810: Cross-Entropy Loss: 0.5466078376400555\n",
      "Step 2820: Cross-Entropy Loss: 0.546607834799599\n",
      "Step 2830: Cross-Entropy Loss: 0.5466078320757926\n",
      "Step 2840: Cross-Entropy Loss: 0.5466078294638456\n",
      "Step 2850: Cross-Entropy Loss: 0.5466078269591642\n",
      "Step 2860: Cross-Entropy Loss: 0.5466078245573422\n",
      "Step 2870: Cross-Entropy Loss: 0.5466078222541562\n",
      "Step 2880: Cross-Entropy Loss: 0.5466078200455546\n",
      "Step 2890: Cross-Entropy Loss: 0.5466078179276528\n",
      "Step 2900: Cross-Entropy Loss: 0.5466078158967266\n",
      "Step 2910: Cross-Entropy Loss: 0.5466078139492032\n",
      "Step 2920: Cross-Entropy Loss: 0.5466078120816578\n",
      "Step 2930: Cross-Entropy Loss: 0.5466078102908055\n",
      "Step 2940: Cross-Entropy Loss: 0.5466078085734967\n",
      "Step 2950: Cross-Entropy Loss: 0.5466078069267112\n",
      "Step 2960: Cross-Entropy Loss: 0.5466078053475528\n",
      "Step 2970: Cross-Entropy Loss: 0.5466078038332441\n",
      "Step 2980: Cross-Entropy Loss: 0.5466078023811216\n",
      "Step 2990: Cross-Entropy Loss: 0.5466078009886319\n",
      "Step 3000: Cross-Entropy Loss: 0.5466077996533256\n",
      "Step 3010: Cross-Entropy Loss: 0.5466077983728544\n",
      "Step 3020: Cross-Entropy Loss: 0.5466077971449668\n",
      "Step 3030: Cross-Entropy Loss: 0.5466077959675025\n",
      "Step 3040: Cross-Entropy Loss: 0.5466077948383917\n",
      "Step 3050: Cross-Entropy Loss: 0.5466077937556479\n",
      "Step 3060: Cross-Entropy Loss: 0.5466077927173671\n",
      "Step 3070: Cross-Entropy Loss: 0.5466077917217238\n",
      "Step 3080: Cross-Entropy Loss: 0.5466077907669669\n",
      "Step 3090: Cross-Entropy Loss: 0.5466077898514166\n",
      "Step 3100: Cross-Entropy Loss: 0.5466077889734634\n",
      "Step 3110: Cross-Entropy Loss: 0.5466077881315633\n",
      "Step 3120: Cross-Entropy Loss: 0.5466077873242356\n",
      "Step 3130: Cross-Entropy Loss: 0.5466077865500607\n",
      "Step 3140: Cross-Entropy Loss: 0.5466077858076773\n",
      "Step 3150: Cross-Entropy Loss: 0.5466077850957793\n",
      "Step 3160: Cross-Entropy Loss: 0.5466077844131153\n",
      "Step 3170: Cross-Entropy Loss: 0.5466077837584843\n",
      "Step 3180: Cross-Entropy Loss: 0.5466077831307355\n",
      "Step 3190: Cross-Entropy Loss: 0.5466077825287649\n",
      "Step 3200: Cross-Entropy Loss: 0.5466077819515137\n",
      "Step 3210: Cross-Entropy Loss: 0.5466077813979672\n",
      "Step 3220: Cross-Entropy Loss: 0.5466077808671512\n",
      "Step 3230: Cross-Entropy Loss: 0.546607780358133\n",
      "Step 3240: Cross-Entropy Loss: 0.5466077798700171\n",
      "Step 3250: Cross-Entropy Loss: 0.5466077794019453\n",
      "Step 3260: Cross-Entropy Loss: 0.5466077789530944\n",
      "Step 3270: Cross-Entropy Loss: 0.5466077785226752\n",
      "Step 3280: Cross-Entropy Loss: 0.5466077781099306\n",
      "Step 3290: Cross-Entropy Loss: 0.5466077777141349\n",
      "Step 3300: Cross-Entropy Loss: 0.546607777334592\n",
      "Step 3310: Cross-Entropy Loss: 0.5466077769706347\n",
      "Step 3320: Cross-Entropy Loss: 0.5466077766216229\n",
      "Step 3330: Cross-Entropy Loss: 0.5466077762869427\n",
      "Step 3340: Cross-Entropy Loss: 0.5466077759660054\n",
      "Step 3350: Cross-Entropy Loss: 0.5466077756582476\n",
      "Step 3360: Cross-Entropy Loss: 0.5466077753631273\n",
      "Step 3370: Cross-Entropy Loss: 0.5466077750801251\n",
      "Step 3380: Cross-Entropy Loss: 0.5466077748087446\n",
      "Step 3390: Cross-Entropy Loss: 0.5466077745485077\n",
      "Step 3400: Cross-Entropy Loss: 0.546607774298957\n",
      "Step 3410: Cross-Entropy Loss: 0.5466077740596538\n",
      "Step 3420: Cross-Entropy Loss: 0.5466077738301772\n",
      "Step 3430: Cross-Entropy Loss: 0.5466077736101236\n",
      "Step 3440: Cross-Entropy Loss: 0.5466077733991062\n",
      "Step 3450: Cross-Entropy Loss: 0.5466077731967539\n",
      "Step 3460: Cross-Entropy Loss: 0.5466077730027107\n",
      "Step 3470: Cross-Entropy Loss: 0.5466077728166355\n",
      "Step 3480: Cross-Entropy Loss: 0.5466077726382013\n",
      "Step 3490: Cross-Entropy Loss: 0.546607772467094\n",
      "Step 3500: Cross-Entropy Loss: 0.546607772303013\n",
      "Step 3510: Cross-Entropy Loss: 0.5466077721456697\n",
      "Step 3520: Cross-Entropy Loss: 0.5466077719947874\n",
      "Step 3530: Cross-Entropy Loss: 0.5466077718501007\n",
      "Step 3540: Cross-Entropy Loss: 0.5466077717113551\n",
      "Step 3550: Cross-Entropy Loss: 0.5466077715783072\n",
      "Step 3560: Cross-Entropy Loss: 0.5466077714507224\n",
      "Step 3570: Cross-Entropy Loss: 0.5466077713283767\n",
      "Step 3580: Cross-Entropy Loss: 0.5466077712110549\n",
      "Step 3590: Cross-Entropy Loss: 0.5466077710985506\n",
      "Step 3600: Cross-Entropy Loss: 0.5466077709906662\n",
      "Step 3610: Cross-Entropy Loss: 0.5466077708872116\n",
      "Step 3620: Cross-Entropy Loss: 0.5466077707880052\n",
      "Step 3630: Cross-Entropy Loss: 0.5466077706928724\n",
      "Step 3640: Cross-Entropy Loss: 0.5466077706016463\n",
      "Step 3650: Cross-Entropy Loss: 0.5466077705141661\n",
      "Step 3660: Cross-Entropy Loss: 0.5466077704302779\n",
      "Step 3670: Cross-Entropy Loss: 0.5466077703498347\n",
      "Step 3680: Cross-Entropy Loss: 0.5466077702726944\n",
      "Step 3690: Cross-Entropy Loss: 0.5466077701987215\n",
      "Step 3700: Cross-Entropy Loss: 0.5466077701277866\n",
      "Step 3710: Cross-Entropy Loss: 0.5466077700597642\n",
      "Step 3720: Cross-Entropy Loss: 0.5466077699945351\n",
      "Step 3730: Cross-Entropy Loss: 0.5466077699319845\n",
      "Step 3740: Cross-Entropy Loss: 0.5466077698720022\n",
      "Step 3750: Cross-Entropy Loss: 0.5466077698144831\n",
      "Step 3760: Cross-Entropy Loss: 0.5466077697593259\n",
      "Step 3770: Cross-Entropy Loss: 0.5466077697064335\n",
      "Step 3780: Cross-Entropy Loss: 0.546607769655713\n",
      "Step 3790: Cross-Entropy Loss: 0.5466077696070752\n",
      "Step 3800: Cross-Entropy Loss: 0.5466077695604347\n",
      "Step 3810: Cross-Entropy Loss: 0.5466077695157094\n",
      "Step 3820: Cross-Entropy Loss: 0.5466077694728204\n",
      "Step 3830: Cross-Entropy Loss: 0.5466077694316924\n",
      "Step 3840: Cross-Entropy Loss: 0.5466077693922535\n",
      "Step 3850: Cross-Entropy Loss: 0.546607769354434\n",
      "Step 3860: Cross-Entropy Loss: 0.5466077693181676\n",
      "Step 3870: Cross-Entropy Loss: 0.5466077692833902\n",
      "Step 3880: Cross-Entropy Loss: 0.5466077692500411\n",
      "Step 3890: Cross-Entropy Loss: 0.546607769218061\n",
      "Step 3900: Cross-Entropy Loss: 0.5466077691873943\n",
      "Step 3910: Cross-Entropy Loss: 0.5466077691579867\n",
      "Step 3920: Cross-Entropy Loss: 0.5466077691297867\n",
      "Step 3930: Cross-Entropy Loss: 0.5466077691027447\n",
      "Step 3940: Cross-Entropy Loss: 0.5466077690768133\n",
      "Step 3950: Cross-Entropy Loss: 0.5466077690519464\n",
      "Step 3960: Cross-Entropy Loss: 0.5466077690281007\n",
      "Step 3970: Cross-Entropy Loss: 0.5466077690052342\n",
      "Step 3980: Cross-Entropy Loss: 0.5466077689833065\n",
      "Step 3990: Cross-Entropy Loss: 0.5466077689622795\n",
      "Step 4000: Cross-Entropy Loss: 0.5466077689421157\n",
      "Step 4010: Cross-Entropy Loss: 0.5466077689227798\n",
      "Step 4020: Cross-Entropy Loss: 0.5466077689042381\n",
      "Step 4030: Cross-Entropy Loss: 0.5466077688864576\n",
      "Step 4040: Cross-Entropy Loss: 0.5466077688694073\n",
      "Step 4050: Cross-Entropy Loss: 0.546607768853057\n",
      "Step 4060: Cross-Entropy Loss: 0.5466077688373782\n",
      "Step 4070: Cross-Entropy Loss: 0.546607768822343\n",
      "Step 4080: Cross-Entropy Loss: 0.5466077688079254\n",
      "Step 4090: Cross-Entropy Loss: 0.5466077687940997\n",
      "Step 4100: Cross-Entropy Loss: 0.5466077687808419\n",
      "Step 4110: Cross-Entropy Loss: 0.5466077687681283\n",
      "Step 4120: Cross-Entropy Loss: 0.5466077687559366\n",
      "Step 4130: Cross-Entropy Loss: 0.5466077687442459\n",
      "Step 4140: Cross-Entropy Loss: 0.5466077687330351\n",
      "Step 4150: Cross-Entropy Loss: 0.5466077687222844\n",
      "Step 4160: Cross-Entropy Loss: 0.5466077687119756\n",
      "Step 4170: Cross-Entropy Loss: 0.5466077687020897\n",
      "Step 4180: Cross-Entropy Loss: 0.5466077686926099\n",
      "Step 4190: Cross-Entropy Loss: 0.5466077686835193\n",
      "Step 4200: Cross-Entropy Loss: 0.5466077686748019\n",
      "Step 4210: Cross-Entropy Loss: 0.5466077686664426\n",
      "Step 4220: Cross-Entropy Loss: 0.5466077686584269\n",
      "Step 4230: Cross-Entropy Loss: 0.5466077686507396\n",
      "Step 4240: Cross-Entropy Loss: 0.5466077686433686\n",
      "Step 4250: Cross-Entropy Loss: 0.5466077686362997\n",
      "Step 4260: Cross-Entropy Loss: 0.5466077686295213\n",
      "Step 4270: Cross-Entropy Loss: 0.5466077686230213\n",
      "Step 4280: Cross-Entropy Loss: 0.5466077686167883\n",
      "Step 4290: Cross-Entropy Loss: 0.546607768610811\n",
      "Step 4300: Cross-Entropy Loss: 0.5466077686050793\n",
      "Step 4310: Cross-Entropy Loss: 0.5466077685995828\n",
      "Step 4320: Cross-Entropy Loss: 0.5466077685943121\n",
      "Step 4330: Cross-Entropy Loss: 0.546607768589258\n",
      "Step 4340: Cross-Entropy Loss: 0.5466077685844114\n",
      "Step 4350: Cross-Entropy Loss: 0.5466077685797637\n",
      "Step 4360: Cross-Entropy Loss: 0.5466077685753068\n",
      "Step 4370: Cross-Entropy Loss: 0.5466077685710329\n",
      "Step 4380: Cross-Entropy Loss: 0.5466077685669346\n",
      "Step 4390: Cross-Entropy Loss: 0.5466077685630044\n",
      "Step 4400: Cross-Entropy Loss: 0.5466077685592357\n",
      "Step 4410: Cross-Entropy Loss: 0.5466077685556217\n",
      "Step 4420: Cross-Entropy Loss: 0.5466077685521561\n",
      "Step 4430: Cross-Entropy Loss: 0.5466077685488331\n",
      "Step 4440: Cross-Entropy Loss: 0.5466077685456462\n",
      "Step 4450: Cross-Entropy Loss: 0.5466077685425902\n",
      "Step 4460: Cross-Entropy Loss: 0.5466077685396596\n",
      "Step 4470: Cross-Entropy Loss: 0.5466077685368494\n",
      "Step 4480: Cross-Entropy Loss: 0.5466077685341548\n",
      "Step 4490: Cross-Entropy Loss: 0.5466077685315709\n",
      "Step 4500: Cross-Entropy Loss: 0.546607768529093\n",
      "Step 4510: Cross-Entropy Loss: 0.5466077685267167\n",
      "Step 4520: Cross-Entropy Loss: 0.5466077685244379\n",
      "Step 4530: Cross-Entropy Loss: 0.5466077685222528\n",
      "Step 4540: Cross-Entropy Loss: 0.5466077685201576\n",
      "Step 4550: Cross-Entropy Loss: 0.5466077685181481\n",
      "Step 4560: Cross-Entropy Loss: 0.5466077685162213\n",
      "Step 4570: Cross-Entropy Loss: 0.5466077685143735\n",
      "Step 4580: Cross-Entropy Loss: 0.5466077685126017\n",
      "Step 4590: Cross-Entropy Loss: 0.5466077685109028\n",
      "Step 4600: Cross-Entropy Loss: 0.5466077685092734\n",
      "Step 4610: Cross-Entropy Loss: 0.5466077685077109\n",
      "Step 4620: Cross-Entropy Loss: 0.5466077685062127\n",
      "Step 4630: Cross-Entropy Loss: 0.5466077685047759\n",
      "Step 4640: Cross-Entropy Loss: 0.5466077685033982\n",
      "Step 4650: Cross-Entropy Loss: 0.546607768502077\n",
      "Step 4660: Cross-Entropy Loss: 0.5466077685008103\n",
      "Step 4670: Cross-Entropy Loss: 0.5466077684995952\n",
      "Step 4680: Cross-Entropy Loss: 0.5466077684984303\n",
      "Step 4690: Cross-Entropy Loss: 0.5466077684973131\n",
      "Step 4700: Cross-Entropy Loss: 0.5466077684962419\n",
      "Step 4710: Cross-Entropy Loss: 0.5466077684952145\n",
      "Step 4720: Cross-Entropy Loss: 0.5466077684942294\n",
      "Step 4730: Cross-Entropy Loss: 0.5466077684932846\n",
      "Step 4740: Cross-Entropy Loss: 0.546607768492379\n",
      "Step 4750: Cross-Entropy Loss: 0.5466077684915102\n",
      "Step 4760: Cross-Entropy Loss: 0.5466077684906769\n",
      "Step 4770: Cross-Entropy Loss: 0.5466077684898784\n",
      "Step 4780: Cross-Entropy Loss: 0.5466077684891124\n",
      "Step 4790: Cross-Entropy Loss: 0.5466077684883779\n",
      "Step 4800: Cross-Entropy Loss: 0.5466077684876732\n",
      "Step 4810: Cross-Entropy Loss: 0.5466077684869978\n",
      "Step 4820: Cross-Entropy Loss: 0.5466077684863501\n",
      "Step 4830: Cross-Entropy Loss: 0.5466077684857291\n",
      "Step 4840: Cross-Entropy Loss: 0.5466077684851335\n",
      "Step 4850: Cross-Entropy Loss: 0.5466077684845624\n",
      "Step 4860: Cross-Entropy Loss: 0.5466077684840143\n",
      "Step 4870: Cross-Entropy Loss: 0.5466077684834892\n",
      "Step 4880: Cross-Entropy Loss: 0.5466077684829858\n",
      "Step 4890: Cross-Entropy Loss: 0.5466077684825027\n",
      "Step 4900: Cross-Entropy Loss: 0.5466077684820395\n",
      "Step 4910: Cross-Entropy Loss: 0.5466077684815953\n",
      "Step 4920: Cross-Entropy Loss: 0.5466077684811693\n",
      "Step 4930: Cross-Entropy Loss: 0.546607768480761\n",
      "Step 4940: Cross-Entropy Loss: 0.5466077684803694\n",
      "Step 4950: Cross-Entropy Loss: 0.5466077684799938\n",
      "Step 4960: Cross-Entropy Loss: 0.5466077684796337\n",
      "Step 4970: Cross-Entropy Loss: 0.5466077684792884\n",
      "Step 4980: Cross-Entropy Loss: 0.5466077684789571\n",
      "Step 4990: Cross-Entropy Loss: 0.5466077684786396\n",
      "Step 5000: Cross-Entropy Loss: 0.5466077684783351\n",
      "Step 5010: Cross-Entropy Loss: 0.5466077684780432\n",
      "Step 5020: Cross-Entropy Loss: 0.546607768477763\n",
      "Step 5030: Cross-Entropy Loss: 0.5466077684774943\n",
      "Step 5040: Cross-Entropy Loss: 0.5466077684772369\n",
      "Step 5050: Cross-Entropy Loss: 0.5466077684769901\n",
      "Step 5060: Cross-Entropy Loss: 0.5466077684767533\n",
      "Step 5070: Cross-Entropy Loss: 0.5466077684765263\n",
      "Step 5080: Cross-Entropy Loss: 0.5466077684763085\n",
      "Step 5090: Cross-Entropy Loss: 0.5466077684760998\n",
      "Step 5100: Cross-Entropy Loss: 0.5466077684758994\n",
      "Step 5110: Cross-Entropy Loss: 0.5466077684757074\n",
      "Step 5120: Cross-Entropy Loss: 0.5466077684755233\n",
      "Step 5130: Cross-Entropy Loss: 0.5466077684753466\n",
      "Step 5140: Cross-Entropy Loss: 0.5466077684751774\n",
      "Step 5150: Cross-Entropy Loss: 0.5466077684750151\n",
      "Step 5160: Cross-Entropy Loss: 0.5466077684748594\n",
      "Step 5170: Cross-Entropy Loss: 0.5466077684747102\n",
      "Step 5180: Cross-Entropy Loss: 0.5466077684745668\n",
      "Step 5190: Cross-Entropy Loss: 0.5466077684744297\n",
      "Step 5200: Cross-Entropy Loss: 0.5466077684742979\n",
      "Step 5210: Cross-Entropy Loss: 0.5466077684741718\n",
      "Step 5220: Cross-Entropy Loss: 0.5466077684740507\n",
      "Step 5230: Cross-Entropy Loss: 0.5466077684739344\n",
      "Step 5240: Cross-Entropy Loss: 0.5466077684738231\n",
      "Step 5250: Cross-Entropy Loss: 0.5466077684737164\n",
      "Step 5260: Cross-Entropy Loss: 0.5466077684736139\n",
      "Step 5270: Cross-Entropy Loss: 0.5466077684735157\n",
      "Step 5280: Cross-Entropy Loss: 0.5466077684734215\n",
      "Step 5290: Cross-Entropy Loss: 0.5466077684733314\n",
      "Step 5300: Cross-Entropy Loss: 0.546607768473245\n",
      "Step 5310: Cross-Entropy Loss: 0.5466077684731618\n",
      "Step 5320: Cross-Entropy Loss: 0.5466077684730823\n",
      "Step 5330: Cross-Entropy Loss: 0.5466077684730059\n",
      "Step 5340: Cross-Entropy Loss: 0.5466077684729328\n",
      "Step 5350: Cross-Entropy Loss: 0.5466077684728625\n",
      "Step 5360: Cross-Entropy Loss: 0.5466077684727951\n",
      "Step 5370: Cross-Entropy Loss: 0.5466077684727307\n",
      "Step 5380: Cross-Entropy Loss: 0.5466077684726688\n",
      "Step 5390: Cross-Entropy Loss: 0.5466077684726093\n",
      "Step 5400: Cross-Entropy Loss: 0.5466077684725524\n",
      "Step 5410: Cross-Entropy Loss: 0.5466077684724977\n",
      "Step 5420: Cross-Entropy Loss: 0.5466077684724455\n",
      "Step 5430: Cross-Entropy Loss: 0.5466077684723952\n",
      "Step 5440: Cross-Entropy Loss: 0.5466077684723474\n",
      "Step 5450: Cross-Entropy Loss: 0.5466077684723011\n",
      "Step 5460: Cross-Entropy Loss: 0.5466077684722569\n",
      "Step 5470: Cross-Entropy Loss: 0.5466077684722145\n",
      "Step 5480: Cross-Entropy Loss: 0.5466077684721737\n",
      "Step 5490: Cross-Entropy Loss: 0.5466077684721347\n",
      "Step 5500: Cross-Entropy Loss: 0.5466077684720974\n",
      "Step 5510: Cross-Entropy Loss: 0.5466077684720613\n",
      "Step 5520: Cross-Entropy Loss: 0.546607768472027\n",
      "Step 5530: Cross-Entropy Loss: 0.546607768471994\n",
      "Step 5540: Cross-Entropy Loss: 0.5466077684719622\n",
      "Step 5550: Cross-Entropy Loss: 0.546607768471932\n",
      "Step 5560: Cross-Entropy Loss: 0.546607768471903\n",
      "Step 5570: Cross-Entropy Loss: 0.5466077684718751\n",
      "Step 5580: Cross-Entropy Loss: 0.5466077684718482\n",
      "Step 5590: Cross-Entropy Loss: 0.5466077684718226\n",
      "Step 5600: Cross-Entropy Loss: 0.5466077684717978\n",
      "Step 5610: Cross-Entropy Loss: 0.5466077684717742\n",
      "Step 5620: Cross-Entropy Loss: 0.5466077684717516\n",
      "Step 5630: Cross-Entropy Loss: 0.5466077684717299\n",
      "Step 5640: Cross-Entropy Loss: 0.5466077684717091\n",
      "Step 5650: Cross-Entropy Loss: 0.5466077684716892\n",
      "Step 5660: Cross-Entropy Loss: 0.5466077684716701\n",
      "Step 5670: Cross-Entropy Loss: 0.5466077684716518\n",
      "Step 5680: Cross-Entropy Loss: 0.5466077684716342\n",
      "Step 5690: Cross-Entropy Loss: 0.5466077684716173\n",
      "Step 5700: Cross-Entropy Loss: 0.5466077684716011\n",
      "Step 5710: Cross-Entropy Loss: 0.5466077684715858\n",
      "Step 5720: Cross-Entropy Loss: 0.5466077684715708\n",
      "Step 5730: Cross-Entropy Loss: 0.5466077684715566\n",
      "Step 5740: Cross-Entropy Loss: 0.5466077684715426\n",
      "Step 5750: Cross-Entropy Loss: 0.5466077684715296\n",
      "Step 5760: Cross-Entropy Loss: 0.546607768471517\n",
      "Step 5770: Cross-Entropy Loss: 0.5466077684715052\n",
      "Step 5780: Cross-Entropy Loss: 0.5466077684714934\n",
      "Step 5790: Cross-Entropy Loss: 0.5466077684714823\n",
      "Step 5800: Cross-Entropy Loss: 0.5466077684714717\n",
      "Step 5810: Cross-Entropy Loss: 0.5466077684714616\n",
      "Step 5820: Cross-Entropy Loss: 0.5466077684714518\n",
      "Step 5830: Cross-Entropy Loss: 0.5466077684714423\n",
      "Step 5840: Cross-Entropy Loss: 0.5466077684714333\n",
      "Step 5850: Cross-Entropy Loss: 0.5466077684714247\n",
      "Step 5860: Cross-Entropy Loss: 0.5466077684714163\n",
      "Step 5870: Cross-Entropy Loss: 0.5466077684714085\n",
      "Step 5880: Cross-Entropy Loss: 0.546607768471401\n",
      "Step 5890: Cross-Entropy Loss: 0.5466077684713935\n",
      "Step 5900: Cross-Entropy Loss: 0.5466077684713867\n",
      "Step 5910: Cross-Entropy Loss: 0.5466077684713797\n",
      "Step 5920: Cross-Entropy Loss: 0.5466077684713733\n",
      "Step 5930: Cross-Entropy Loss: 0.5466077684713673\n",
      "Step 5940: Cross-Entropy Loss: 0.5466077684713615\n",
      "Step 5950: Cross-Entropy Loss: 0.5466077684713556\n",
      "Step 5960: Cross-Entropy Loss: 0.5466077684713503\n",
      "Step 5970: Cross-Entropy Loss: 0.5466077684713452\n",
      "Step 5980: Cross-Entropy Loss: 0.5466077684713401\n",
      "Step 5990: Cross-Entropy Loss: 0.5466077684713353\n",
      "Step 6000: Cross-Entropy Loss: 0.5466077684713306\n",
      "Step 6010: Cross-Entropy Loss: 0.5466077684713261\n",
      "Step 6020: Cross-Entropy Loss: 0.546607768471322\n",
      "Step 6030: Cross-Entropy Loss: 0.5466077684713179\n",
      "Step 6040: Cross-Entropy Loss: 0.546607768471314\n",
      "Step 6050: Cross-Entropy Loss: 0.5466077684713103\n",
      "Step 6060: Cross-Entropy Loss: 0.5466077684713069\n",
      "Step 6070: Cross-Entropy Loss: 0.5466077684713033\n",
      "Step 6080: Cross-Entropy Loss: 0.5466077684713001\n",
      "Step 6090: Cross-Entropy Loss: 0.5466077684712969\n",
      "Step 6100: Cross-Entropy Loss: 0.5466077684712939\n",
      "Step 6110: Cross-Entropy Loss: 0.5466077684712911\n",
      "Step 6120: Cross-Entropy Loss: 0.5466077684712883\n",
      "Step 6130: Cross-Entropy Loss: 0.5466077684712856\n",
      "Step 6140: Cross-Entropy Loss: 0.5466077684712829\n",
      "Step 6150: Cross-Entropy Loss: 0.5466077684712806\n",
      "Step 6160: Cross-Entropy Loss: 0.5466077684712785\n",
      "Step 6170: Cross-Entropy Loss: 0.546607768471276\n",
      "Step 6180: Cross-Entropy Loss: 0.5466077684712738\n",
      "Step 6190: Cross-Entropy Loss: 0.5466077684712716\n",
      "Step 6200: Cross-Entropy Loss: 0.5466077684712696\n",
      "Step 6210: Cross-Entropy Loss: 0.5466077684712678\n",
      "Step 6220: Cross-Entropy Loss: 0.5466077684712659\n",
      "Step 6230: Cross-Entropy Loss: 0.5466077684712642\n",
      "Step 6240: Cross-Entropy Loss: 0.5466077684712625\n",
      "Step 6250: Cross-Entropy Loss: 0.5466077684712609\n",
      "Step 6260: Cross-Entropy Loss: 0.5466077684712595\n",
      "Step 6270: Cross-Entropy Loss: 0.5466077684712578\n",
      "Step 6280: Cross-Entropy Loss: 0.5466077684712565\n",
      "Step 6290: Cross-Entropy Loss: 0.546607768471255\n",
      "Step 6300: Cross-Entropy Loss: 0.5466077684712538\n",
      "Step 6310: Cross-Entropy Loss: 0.5466077684712525\n",
      "Step 6320: Cross-Entropy Loss: 0.5466077684712515\n",
      "Step 6330: Cross-Entropy Loss: 0.5466077684712501\n",
      "Step 6340: Cross-Entropy Loss: 0.546607768471249\n",
      "Step 6350: Cross-Entropy Loss: 0.546607768471248\n",
      "Step 6360: Cross-Entropy Loss: 0.5466077684712468\n",
      "Step 6370: Cross-Entropy Loss: 0.546607768471246\n",
      "Step 6380: Cross-Entropy Loss: 0.546607768471245\n",
      "Step 6390: Cross-Entropy Loss: 0.5466077684712443\n",
      "Step 6400: Cross-Entropy Loss: 0.5466077684712433\n",
      "Step 6410: Cross-Entropy Loss: 0.5466077684712425\n",
      "Step 6420: Cross-Entropy Loss: 0.5466077684712417\n",
      "Step 6430: Cross-Entropy Loss: 0.5466077684712409\n",
      "Step 6440: Cross-Entropy Loss: 0.5466077684712402\n",
      "Step 6450: Cross-Entropy Loss: 0.5466077684712396\n",
      "Step 6460: Cross-Entropy Loss: 0.5466077684712389\n",
      "Step 6470: Cross-Entropy Loss: 0.5466077684712383\n",
      "Step 6480: Cross-Entropy Loss: 0.5466077684712376\n",
      "Step 6490: Cross-Entropy Loss: 0.546607768471237\n",
      "Step 6500: Cross-Entropy Loss: 0.5466077684712365\n",
      "Step 6510: Cross-Entropy Loss: 0.546607768471236\n",
      "Step 6520: Cross-Entropy Loss: 0.5466077684712355\n",
      "Step 6530: Cross-Entropy Loss: 0.546607768471235\n",
      "Step 6540: Cross-Entropy Loss: 0.5466077684712344\n",
      "Step 6550: Cross-Entropy Loss: 0.546607768471234\n",
      "Step 6560: Cross-Entropy Loss: 0.5466077684712335\n",
      "Step 6570: Cross-Entropy Loss: 0.5466077684712334\n",
      "Step 6580: Cross-Entropy Loss: 0.5466077684712326\n",
      "Step 6590: Cross-Entropy Loss: 0.5466077684712323\n",
      "Step 6600: Cross-Entropy Loss: 0.5466077684712319\n",
      "Step 6610: Cross-Entropy Loss: 0.5466077684712315\n",
      "Step 6620: Cross-Entropy Loss: 0.5466077684712313\n",
      "Step 6630: Cross-Entropy Loss: 0.5466077684712308\n",
      "Step 6640: Cross-Entropy Loss: 0.5466077684712306\n",
      "Step 6650: Cross-Entropy Loss: 0.5466077684712303\n",
      "Step 6660: Cross-Entropy Loss: 0.54660776847123\n",
      "Step 6670: Cross-Entropy Loss: 0.5466077684712298\n",
      "Step 6680: Cross-Entropy Loss: 0.5466077684712294\n",
      "Step 6690: Cross-Entropy Loss: 0.5466077684712292\n",
      "Step 6700: Cross-Entropy Loss: 0.5466077684712289\n",
      "Step 6710: Cross-Entropy Loss: 0.5466077684712287\n",
      "Step 6720: Cross-Entropy Loss: 0.5466077684712285\n",
      "Step 6730: Cross-Entropy Loss: 0.5466077684712283\n",
      "Step 6740: Cross-Entropy Loss: 0.546607768471228\n",
      "Step 6750: Cross-Entropy Loss: 0.5466077684712279\n",
      "Step 6760: Cross-Entropy Loss: 0.5466077684712276\n",
      "Step 6770: Cross-Entropy Loss: 0.5466077684712276\n",
      "Step 6780: Cross-Entropy Loss: 0.5466077684712272\n",
      "Step 6790: Cross-Entropy Loss: 0.546607768471227\n",
      "Step 6800: Cross-Entropy Loss: 0.546607768471227\n",
      "Step 6810: Cross-Entropy Loss: 0.5466077684712268\n",
      "Step 6820: Cross-Entropy Loss: 0.5466077684712267\n",
      "Step 6830: Cross-Entropy Loss: 0.5466077684712266\n",
      "Step 6840: Cross-Entropy Loss: 0.5466077684712263\n",
      "Step 6850: Cross-Entropy Loss: 0.5466077684712263\n",
      "Step 6860: Cross-Entropy Loss: 0.5466077684712263\n",
      "Step 6870: Cross-Entropy Loss: 0.5466077684712262\n",
      "Step 6880: Cross-Entropy Loss: 0.5466077684712258\n",
      "Step 6890: Cross-Entropy Loss: 0.5466077684712258\n",
      "Step 6900: Cross-Entropy Loss: 0.5466077684712257\n",
      "Step 6910: Cross-Entropy Loss: 0.5466077684712257\n",
      "Step 6920: Cross-Entropy Loss: 0.5466077684712255\n",
      "Step 6930: Cross-Entropy Loss: 0.5466077684712255\n",
      "Step 6940: Cross-Entropy Loss: 0.5466077684712254\n",
      "Step 6950: Cross-Entropy Loss: 0.5466077684712252\n",
      "Step 6960: Cross-Entropy Loss: 0.546607768471225\n",
      "Step 6970: Cross-Entropy Loss: 0.5466077684712253\n",
      "Step 6980: Cross-Entropy Loss: 0.5466077684712252\n",
      "Step 6990: Cross-Entropy Loss: 0.5466077684712248\n",
      "Step 7000: Cross-Entropy Loss: 0.5466077684712248\n",
      "Step 7010: Cross-Entropy Loss: 0.5466077684712248\n",
      "Step 7020: Cross-Entropy Loss: 0.5466077684712247\n",
      "Step 7030: Cross-Entropy Loss: 0.5466077684712247\n",
      "Step 7040: Cross-Entropy Loss: 0.5466077684712247\n",
      "Step 7050: Cross-Entropy Loss: 0.5466077684712246\n",
      "Step 7060: Cross-Entropy Loss: 0.5466077684712245\n",
      "Step 7070: Cross-Entropy Loss: 0.5466077684712245\n",
      "Step 7080: Cross-Entropy Loss: 0.5466077684712245\n",
      "Step 7090: Cross-Entropy Loss: 0.5466077684712246\n",
      "Step 7100: Cross-Entropy Loss: 0.5466077684712244\n",
      "Step 7110: Cross-Entropy Loss: 0.5466077684712244\n",
      "Step 7120: Cross-Entropy Loss: 0.5466077684712242\n",
      "Step 7130: Cross-Entropy Loss: 0.5466077684712243\n",
      "Step 7140: Cross-Entropy Loss: 0.5466077684712242\n",
      "Step 7150: Cross-Entropy Loss: 0.546607768471224\n",
      "Step 7160: Cross-Entropy Loss: 0.5466077684712242\n",
      "Step 7170: Cross-Entropy Loss: 0.546607768471224\n",
      "Step 7180: Cross-Entropy Loss: 0.546607768471224\n",
      "Step 7190: Cross-Entropy Loss: 0.5466077684712239\n",
      "Step 7200: Cross-Entropy Loss: 0.546607768471224\n",
      "Step 7210: Cross-Entropy Loss: 0.5466077684712238\n",
      "Step 7220: Cross-Entropy Loss: 0.546607768471224\n",
      "Step 7230: Cross-Entropy Loss: 0.5466077684712237\n",
      "Step 7240: Cross-Entropy Loss: 0.5466077684712237\n",
      "Step 7250: Cross-Entropy Loss: 0.5466077684712239\n",
      "Step 7260: Cross-Entropy Loss: 0.5466077684712238\n",
      "Step 7270: Cross-Entropy Loss: 0.5466077684712237\n",
      "Step 7280: Cross-Entropy Loss: 0.5466077684712237\n",
      "Step 7290: Cross-Entropy Loss: 0.5466077684712238\n",
      "Step 7300: Cross-Entropy Loss: 0.5466077684712235\n",
      "Step 7310: Cross-Entropy Loss: 0.5466077684712237\n",
      "Step 7320: Cross-Entropy Loss: 0.5466077684712237\n",
      "Step 7330: Cross-Entropy Loss: 0.5466077684712237\n",
      "Step 7340: Cross-Entropy Loss: 0.5466077684712237\n",
      "Step 7350: Cross-Entropy Loss: 0.5466077684712237\n",
      "Step 7360: Cross-Entropy Loss: 0.5466077684712236\n",
      "Step 7370: Cross-Entropy Loss: 0.5466077684712237\n",
      "Step 7380: Cross-Entropy Loss: 0.5466077684712237\n",
      "Step 7390: Cross-Entropy Loss: 0.5466077684712235\n",
      "Step 7400: Cross-Entropy Loss: 0.5466077684712237\n",
      "Step 7410: Cross-Entropy Loss: 0.5466077684712236\n",
      "Step 7420: Cross-Entropy Loss: 0.5466077684712235\n",
      "Step 7430: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 7440: Cross-Entropy Loss: 0.5466077684712237\n",
      "Step 7450: Cross-Entropy Loss: 0.5466077684712235\n",
      "Step 7460: Cross-Entropy Loss: 0.5466077684712235\n",
      "Step 7470: Cross-Entropy Loss: 0.5466077684712235\n",
      "Step 7480: Cross-Entropy Loss: 0.5466077684712235\n",
      "Step 7490: Cross-Entropy Loss: 0.5466077684712236\n",
      "Step 7500: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 7510: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 7520: Cross-Entropy Loss: 0.5466077684712235\n",
      "Step 7530: Cross-Entropy Loss: 0.5466077684712236\n",
      "Step 7540: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 7550: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 7560: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 7570: Cross-Entropy Loss: 0.5466077684712235\n",
      "Step 7580: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 7590: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 7600: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 7610: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 7620: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 7630: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 7640: Cross-Entropy Loss: 0.5466077684712235\n",
      "Step 7650: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 7660: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 7670: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 7680: Cross-Entropy Loss: 0.5466077684712235\n",
      "Step 7690: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 7700: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 7710: Cross-Entropy Loss: 0.5466077684712235\n",
      "Step 7720: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 7730: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 7740: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 7750: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 7760: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 7770: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 7780: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 7790: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 7800: Cross-Entropy Loss: 0.5466077684712235\n",
      "Step 7810: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 7820: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 7830: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 7840: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 7850: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 7860: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 7870: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 7880: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 7890: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 7900: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 7910: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 7920: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 7930: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 7940: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 7950: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 7960: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 7970: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 7980: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 7990: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 8000: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 8010: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 8020: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 8030: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 8040: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 8050: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 8060: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 8070: Cross-Entropy Loss: 0.546607768471223\n",
      "Step 8080: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 8090: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 8100: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 8110: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 8120: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 8130: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 8140: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 8150: Cross-Entropy Loss: 0.546607768471223\n",
      "Step 8160: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 8170: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 8180: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 8190: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 8200: Cross-Entropy Loss: 0.546607768471223\n",
      "Step 8210: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 8220: Cross-Entropy Loss: 0.546607768471223\n",
      "Step 8230: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 8240: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 8250: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 8260: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 8270: Cross-Entropy Loss: 0.546607768471223\n",
      "Step 8280: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 8290: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 8300: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 8310: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 8320: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 8330: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 8340: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 8350: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 8360: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 8370: Cross-Entropy Loss: 0.546607768471223\n",
      "Step 8380: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 8390: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 8400: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 8410: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 8420: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 8430: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 8440: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 8450: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 8460: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 8470: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 8480: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 8490: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 8500: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 8510: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 8520: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 8530: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 8540: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 8550: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 8560: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 8570: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 8580: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 8590: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 8600: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 8610: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 8620: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 8630: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 8640: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 8650: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 8660: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 8670: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 8680: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 8690: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 8700: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 8710: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 8720: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 8730: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 8740: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 8750: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 8760: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 8770: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 8780: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 8790: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 8800: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 8810: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 8820: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 8830: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 8840: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 8850: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 8860: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 8870: Cross-Entropy Loss: 0.546607768471223\n",
      "Step 8880: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 8890: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 8900: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 8910: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 8920: Cross-Entropy Loss: 0.546607768471223\n",
      "Step 8930: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 8940: Cross-Entropy Loss: 0.546607768471223\n",
      "Step 8950: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 8960: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 8970: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 8980: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 8990: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 9000: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 9010: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 9020: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 9030: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 9040: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 9050: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 9060: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 9070: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 9080: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 9090: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 9100: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 9110: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 9120: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 9130: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 9140: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 9150: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 9160: Cross-Entropy Loss: 0.546607768471223\n",
      "Step 9170: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 9180: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 9190: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 9200: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 9210: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 9220: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 9230: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 9240: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 9250: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 9260: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 9270: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 9280: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 9290: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 9300: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 9310: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 9320: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 9330: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 9340: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 9350: Cross-Entropy Loss: 0.546607768471223\n",
      "Step 9360: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 9370: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 9380: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 9390: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 9400: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 9410: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 9420: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 9430: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 9440: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 9450: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 9460: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 9470: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 9480: Cross-Entropy Loss: 0.5466077684712235\n",
      "Step 9490: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 9500: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 9510: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 9520: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 9530: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 9540: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 9550: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 9560: Cross-Entropy Loss: 0.546607768471223\n",
      "Step 9570: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 9580: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 9590: Cross-Entropy Loss: 0.5466077684712235\n",
      "Step 9600: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 9610: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 9620: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 9630: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 9640: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 9650: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 9660: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 9670: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 9680: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 9690: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 9700: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 9710: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 9720: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 9730: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 9740: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 9750: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 9760: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 9770: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 9780: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 9790: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 9800: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 9810: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 9820: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 9830: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 9840: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 9850: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 9860: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 9870: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 9880: Cross-Entropy Loss: 0.546607768471223\n",
      "Step 9890: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 9900: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 9910: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 9920: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 9930: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 9940: Cross-Entropy Loss: 0.5466077684712234\n",
      "Step 9950: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 9960: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 9970: Cross-Entropy Loss: 0.5466077684712233\n",
      "Step 9980: Cross-Entropy Loss: 0.546607768471223\n",
      "Step 9990: Cross-Entropy Loss: 0.5466077684712232\n",
      "Step 10000: Cross-Entropy Loss: 0.546607768471223\n"
     ]
    }
   ],
   "source": [
    "# initialize and train the model\n",
    "my_model = MyLogisticRegression()\n",
    "\n",
    "i = w.columns[0]\n",
    "X = w[[i, 'date_factor']].values\n",
    "y = S_gt[i].values\n",
    "\n",
    "losses = my_model.fit(X, y)\n",
    "for i, loss in enumerate(losses):\n",
    "    if i%10 ==0:\n",
    "        print(\"Step {}: Cross-Entropy Loss: {}\".format(i, loss)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "水生根茎类 [ 0.87044213 -3.07242384  0.31215144] [0.05644082]\n",
      "花叶类 [ 5.49034117 -7.43220486 -0.38073743] [0.03142249]\n",
      "花菜类 [-0.34783207 -0.37234675  0.73931001] [0.31117149]\n",
      "茄类 [ 0.62168268 -0.74610861 -0.08747892] [0.43201182]\n",
      "辣椒类 [-0.55381326  0.79281676 -0.56631668] [0.59810293]\n",
      "食用菌 [-5.83608669  9.24336588 -0.17446366] [0.99481069]\n"
     ]
    }
   ],
   "source": [
    "#得到七天各自S预测模型\n",
    "S_model = {}\n",
    "for i in S_gt.columns:\n",
    "    X = w[[i, 'date_factor']].values\n",
    "    y = S_gt[i].values\n",
    "    my_model = MyLogisticRegression()\n",
    "    my_model.fit(X, y)\n",
    "    S_model[i] = my_model\n",
    "\n",
    "    x = np.zeros((1,2))\n",
    "    x[0,0] = 1.2\n",
    "    x[0,1] = 0.0\n",
    "    S_model[i].predict(x)\n",
    "    print(i, my_model.beta, S_model[i].predict(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03142248901082407"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = np.zeros((1,2))\n",
    "# x[0,0] = 1.2\n",
    "# x[0,1] = 0.0\n",
    "x = np.array([1.2, 0.0]).reshape(1,2)\n",
    "S_model['花叶类'].predict(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "水生根茎类    0.979524\n",
       "花叶类      0.974655\n",
       "花菜类      0.980625\n",
       "茄类       0.997835\n",
       "辣椒类      0.992385\n",
       "食用菌      0.947664\n",
       "Name: avg_discount, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4.处理得到avg_discount\n",
    "discount_temp1 = price_discount / price_processed\n",
    "missing_col = [col for col in price_processed.columns if col not in price_discount.columns]\n",
    "discount_temp1[missing_col] = 1\n",
    "discount_k = discount_temp1[price_processed.columns].fillna(1)\n",
    "\n",
    "discount_prime = discount_k * sale_product_processed\n",
    "\n",
    "zero = np.zeros((21,6))\n",
    "discount_x_Θ =  pd.DataFrame(zero, columns=sale_category.columns, index=sale_category.index)\n",
    "for col in discount_prime.columns:\n",
    "    discount_x_Θ[info_dict[col]] += discount_prime[col]\n",
    "\n",
    "discount_df = discount_x_Θ / sale_category_processed\n",
    "discount_df.loc['avg_discount'] = discount_df.apply(lambda x:x.sum()/21)\n",
    "discount = discount_df.loc['avg_discount']\n",
    "discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'水生根茎类': 0.0144534142703672,\n",
       " '花叶类': 0.0210318035158078,\n",
       " '花菜类': 0.0082996631173022,\n",
       " '茄类': 0.0020344895778648,\n",
       " '辣椒类': 0.0010031671900583,\n",
       " '食用菌': 0.1148167555404863}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5.处理制作p1,p2的字典\n",
    "p1_dict = p1.set_index('分类名称').to_dict(orient='dict')['0']\n",
    "p2_dict = p2.set_index('分类名称').to_dict(orient='dict')['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\谢嘉楠\\AppData\\Local\\Temp\\ipykernel_14668\\1456669332.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sale_product_processed['date'] = t_np\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>水生根茎类</th>\n",
       "      <th>花叶类</th>\n",
       "      <th>花菜类</th>\n",
       "      <th>茄类</th>\n",
       "      <th>辣椒类</th>\n",
       "      <th>食用菌</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-07-01</th>\n",
       "      <td>9.787288</td>\n",
       "      <td>4.105093</td>\n",
       "      <td>8.475622</td>\n",
       "      <td>3.706847</td>\n",
       "      <td>7.086264</td>\n",
       "      <td>6.539818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-02</th>\n",
       "      <td>9.698288</td>\n",
       "      <td>4.131553</td>\n",
       "      <td>8.644676</td>\n",
       "      <td>3.635820</td>\n",
       "      <td>6.470582</td>\n",
       "      <td>7.066302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-03</th>\n",
       "      <td>6.429283</td>\n",
       "      <td>4.133861</td>\n",
       "      <td>8.519877</td>\n",
       "      <td>3.690829</td>\n",
       "      <td>6.306548</td>\n",
       "      <td>7.545625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-04</th>\n",
       "      <td>7.392045</td>\n",
       "      <td>4.162202</td>\n",
       "      <td>8.558640</td>\n",
       "      <td>3.449725</td>\n",
       "      <td>6.994952</td>\n",
       "      <td>7.128029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-05</th>\n",
       "      <td>7.048889</td>\n",
       "      <td>4.016360</td>\n",
       "      <td>8.286091</td>\n",
       "      <td>3.644148</td>\n",
       "      <td>7.157121</td>\n",
       "      <td>6.753155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-06</th>\n",
       "      <td>6.429663</td>\n",
       "      <td>4.122649</td>\n",
       "      <td>8.311134</td>\n",
       "      <td>3.700899</td>\n",
       "      <td>6.651031</td>\n",
       "      <td>7.540654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-07</th>\n",
       "      <td>5.857301</td>\n",
       "      <td>4.140400</td>\n",
       "      <td>8.339956</td>\n",
       "      <td>3.544302</td>\n",
       "      <td>6.680578</td>\n",
       "      <td>7.226703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               水生根茎类       花叶类       花菜类        茄类       辣椒类       食用菌\n",
       "Unnamed: 0                                                            \n",
       "2023-07-01  9.787288  4.105093  8.475622  3.706847  7.086264  6.539818\n",
       "2023-07-02  9.698288  4.131553  8.644676  3.635820  6.470582  7.066302\n",
       "2023-07-03  6.429283  4.133861  8.519877  3.690829  6.306548  7.545625\n",
       "2023-07-04  7.392045  4.162202  8.558640  3.449725  6.994952  7.128029\n",
       "2023-07-05  7.048889  4.016360  8.286091  3.644148  7.157121  6.753155\n",
       "2023-07-06  6.429663  4.122649  8.311134  3.700899  6.651031  7.540654\n",
       "2023-07-07  5.857301  4.140400  8.339956  3.544302  6.680578  7.226703"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6.计算得到c\n",
    "\n",
    "t_list = []\n",
    "for date in sale_product_processed.index:\n",
    "    time = datetime.datetime.strptime(date, '%Y-%m-%d')\n",
    "    t_list.append(time.day)\n",
    "t_np = np.array(t_list)\n",
    "sale_product_processed['date'] = t_np\n",
    "sale_product_temp = sale_product_processed.groupby('date').sum().values\n",
    "sale_product_processed = sale_product_processed.drop('date', axis=1)\n",
    "\n",
    "t_list = []\n",
    "for date in sale_category_processed.index:\n",
    "    time = datetime.datetime.strptime(date, '%Y-%m-%d')\n",
    "    t_list.append(time.day)\n",
    "t_np = np.array(t_list)\n",
    "sale_category_processed['date'] = t_np\n",
    "sale_category_temp = sale_category_processed.groupby('date').sum().values\n",
    "sale_category_processed = sale_category_processed.drop('date', axis=1)\n",
    "\n",
    "c_prime = c_df * sale_product_temp\n",
    "\n",
    "zero = np.zeros((7,6))\n",
    "c_x_Θ =  pd.DataFrame(zero, columns=sale_category_processed.columns, index=c_prime.index)\n",
    "for col in c_prime.columns:\n",
    "    c_x_Θ[info_dict[col]] += c_prime[col]\n",
    "\n",
    "c = c_x_Θ / sale_category_temp\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'花菜类': 0.1551,\n",
       " '水生根茎类': 0.1365,\n",
       " '花叶类': 0.1283,\n",
       " '食用菌': 0.09449999999999999,\n",
       " '辣椒类': 0.0924,\n",
       " '茄类': 0.0668}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#7.读取yita\n",
    "l = pd.read_excel('附件4.xlsx')\n",
    "l['平均损耗率(%)_小分类编码_不同值'] = l['平均损耗率(%)_小分类编码_不同值'] / 100\n",
    "loss_rate = l[['小分类名称','平均损耗率(%)_小分类编码_不同值']].set_index('小分类名称').to_dict(orient='dict')['平均损耗率(%)_小分类编码_不同值']\n",
    "loss_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>水生根茎类</th>\n",
       "      <th>花叶类</th>\n",
       "      <th>花菜类</th>\n",
       "      <th>茄类</th>\n",
       "      <th>辣椒类</th>\n",
       "      <th>食用菌</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  水生根茎类  花叶类  花菜类   茄类  辣椒类  食用菌\n",
       "0   NaN  NaN  NaN  NaN  NaN  NaN\n",
       "1   NaN  NaN  NaN  NaN  NaN  NaN\n",
       "2   NaN  NaN  NaN  NaN  NaN  NaN\n",
       "3   NaN  NaN  NaN  NaN  NaN  NaN\n",
       "4   NaN  NaN  NaN  NaN  NaN  NaN\n",
       "5   NaN  NaN  NaN  NaN  NaN  NaN\n",
       "6   NaN  NaN  NaN  NaN  NaN  NaN"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.DataFrame(index= [i for i in range(7)], columns= sale_category_processed.columns)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.78457353535324"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = '水生根茎类'\n",
    "day = 0\n",
    "w = 1.2\n",
    "S_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\谢嘉楠\\AppData\\Local\\Temp\\ipykernel_14668\\1200590487.py:38: RuntimeWarning: overflow encountered in double_scalars\n",
      "  if pow(np.e, (f - f_new) / T) > random.random():  # 根据Metropolis准则接受新解\n"
     ]
    }
   ],
   "source": [
    "#结果生成\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "label_list = sale_category_processed.columns \n",
    "day_list = [0,1,2,3,4,5,6]\n",
    "\n",
    "for day in day_list:\n",
    "    for label in label_list:\n",
    "        # 设置参数：温度、最低温度、降温系数、解空间\n",
    "        f_list = []\n",
    "        x_dict = {}\n",
    "        S_dict = {}\n",
    "        for iter in range(20):\n",
    "            T = 20000\n",
    "            Tmin = 1e-4\n",
    "            delta = 0.98\n",
    "            bound = [0, 1.5]\n",
    "            # 定义目标函数\n",
    "            func = lambda w: (p1_dict[label] * (S_model[label].predict(np.array([w, day/6]).reshape(1,2))[0] * (S_max[label] - S_min[label]) + S_min[label]) * (1 + w) * c[label][day] +\n",
    "                            p2_dict[label] * (S_model[label].predict(np.array([w, day/6]).reshape(1,2))[0] * (S_max[label] - S_min[label]) + S_min[label]) * (1 + w) * discount[label] * c[label][day] -\n",
    "                            (S_model[label].predict(np.array([w, day/6]).reshape(1,2))[0] * (S_max[label] - S_min[label]) + S_min[label]) * c[label][day] / (1 - loss_rate[label]))\n",
    "            # 初始值定义（也表示最优值）\n",
    "            x = random.uniform(bound[0], bound[1])\n",
    "            f = func(x)\n",
    "\n",
    "            # epoch = 0\n",
    "            while T > Tmin:\n",
    "                # 产生新解及新函数值\n",
    "                x_new = x + (random.random() * 2 - 1) * T\n",
    "                while not 0 < x_new < 1.5:# 保证生成的解在解空间中，使得每次温度降低是有意义的\n",
    "                    x_new = x + (random.random() * 2 - 1) * T\n",
    "                f_new = func(x_new)\n",
    "                # 计算增量\n",
    "                if f_new > f:  # 以1的概率接受新解\n",
    "                    x, f = x_new, f_new\n",
    "                else:\n",
    "                    if pow(np.e, (f - f_new) / T) > random.random():  # 根据Metropolis准则接受新解\n",
    "                        x, f = x_new, f_new\n",
    "                T *= delta\n",
    "            f_list.append(f)\n",
    "            x_dict[f] = x\n",
    "            S_dict[f] = (S_model[label].predict(np.array([x, day/6]).reshape(1,2))[0] * (S_max[label] - S_min[label]) + S_min[label])/ (1 - loss_rate[label])\n",
    "                # epoch += 1\n",
    "            # print(f'函数的最大值为：{f},此时变量的值为：{x}')\n",
    "        f_max = np.array(f_list).max()\n",
    "        x_max = x_dict[f_max]\n",
    "        S_j_max = S_dict[f_max]\n",
    "        result.loc[day, label] = (f_max, x_max, S_j_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存问题二结果\n",
    "result.to_excel('问题二加成参数与补货量最优方案.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
